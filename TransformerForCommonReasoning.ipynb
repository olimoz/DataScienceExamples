{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.1 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "af407973ba12897262deda9d8992946cc1a9873fff2de40f1acc89cdf9010052"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Building a Transformer Trained on 'Common Reasoning'\n",
    "### By Oliver Morris\n",
    "\n",
    "## Inspiration\n",
    "\n",
    "There is a lot of discussion about deep reasoning and many approaches being attempted. This is often preceded by discussion on causation being different to correlation, with the substantial amount of work being done by Professor Judea Pearl of UCLA, see his book \"Causality: Models, Reasoning and Inference\" which applies graph models to derive causality. See YouTube for a quick introduction! https://www.youtube.com/watch?v=ZaPV1OSEpHw\n",
    "\n",
    "Since there is already so much work based on the graph and causality approach, this model will approach deep reasoning from 'left of field'.\n",
    "\n",
    "The inspiration is GPT-3, the transformer based language model developed by OpenAI. This model excels at translation (mapping) tasks, it does not attempt to learn causation yet somehow it achieves some basic reasoning, albeit flawed: https://cs.nyu.edu/faculty/davise/papers/GPT3CompleteTests.html\n",
    "\n",
    "The best reasoning machines we know of are human beings. Humans make their own way through the world and use experience to improve their assessments of cause and effect. As anyone has taught children knows, these assessments of causality can be inspired, or they can be awfully flawed! Children must be taught to reason effectively and the rigorous reasoning of mathematics does not come easily to many otherwise successful humans. There are many subjects upon which we must make conclusions, but where we struggle to disentangle causation from correlation. The vast majority of us certainly have not mastered the tools for modelling causation statistically, as per Prof Pearl's treatise. Yet, even without these tools we humans can be amazingly adaptable and successful.\n",
    "\n",
    "One of the key motivations for using Prof Pearl's approach is because it can lead to models capable of giving a reason 'why' the answer is 'x'. This would be a giant leap for the usability of deep learning models. Meanwhile, some tools attempt to tame deep learning models by simplifying them into decision trees, showing how the weights of model guide the pinball of an input thru the pachinco machine of the neural network towards an output. This is useful, but not how humans describe their method for reaching a conclusion. \n",
    "\n",
    "Humans give a justification for a conclusion as an output, alongside the conclusion itself. They certainly do not describe the weights of the neurons in their head. To repeat, the justification, which is subtly different to causation, is an output of their 'model' as much as the output itself. We've all given reasons for our conclusions, which initally sound convincing but fail the test of logic. Clearly, our minds are hypothesising reasons for conclusions which are intuitively reached. This justification function is deeply wired in the human brain, we can be tempted into giving justifications for actions we have not even taken, see the split brain experiments. Here it is referred to as 'common reasoning', analogous to 'common sense'.\n",
    "\n",
    "So, this 'left of field' approach is not a challenge to the advantages of statistical causality, not at all. It simply considers the human experience and hypothesises that models based on correlation may have more to give.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## This Model\n",
    "\n",
    "A Transormer is proposed which is trained on pairs of clauses. The input is a clause describing an effect, the target is a cause of that effect. For example, \"The chicken crossed the road, because it was hungry\". The clause up to the word 'because' is the effect, the clause after the word 'because' is the cause. In this way the model is trained to hypothesise justifications (causes) for the effect. The term justification is preferred because the model is not really learning causality, how can we know why the chicken crossed the road? Instead, it is learning plausible justifications, just as people do.\n",
    "\n",
    "Such justifications are hypotheses which could then be tested by another model which is trained using counter factuals, as per Professor Judea. But this implies a reinforcement model, an area which will not be explored here. \n",
    "\n",
    "Let's return to the justification 'transformer'. The data for training will be taken from the 2019 wikipedia dump, Wiki40b, available from huggingface.co and a standard dataset with Tensorflow Datasets. Wiki40b is parsed for sentences with the word 'because' or 'therefore'. Those sentences are then broken down into the cause and effect part of the sentence. The clauses are then embedded using BERT and presented to the Transformer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Future Model\n",
    "\n",
    "Professor Pearl's work gives three levels to developing causal models; \n",
    "\n",
    "- *Association*. Learning by observing the data and its correlations. This is where transformers find themselves\n",
    "\n",
    "- *Intervention*. Interacting with the environment to see what happens. Reinforcement learning is trained by interaction\n",
    "\n",
    "- *Counterfactuals*. Hypothesis testing. Was it X that caused Y? What happens if X is removed from the mix?\n",
    "\n",
    "This transformer is purely on level 1, learning by associations. Nevertheless, consider the first Word2Vec word embeddings which where the word vectors were positioned relative to each other such that we could perform 'word algebra'. For example, Queen = King - Man + Woman. It would be interesting to see whether we can perform similar operations on causes. BERT allows us to embed entire clauses, effects or causes, as single vectors upon which we could hope to apply such operations.\n",
    "\n",
    "It would be especially interesting to analyse the changes due to counterfactuals in such a space. Consider the 'effect' clauses which have only one verb, we could prefix that verb with 'not' and expect the 'cause' to be substantially distanced from the same effect without 'not'. For example, consider our original example, \"Why did the chicken cross the road\" and we get \"Because he was hungry\". Let's negate the verb in the effect: \"Why did the chicken *NOT* cross the road?\" we'd expect something like \"Because he was now full\". We'd hope the two causes were dissimilar, a model could be refined by minimising the similarity in such training examples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\OliverMorris\\OneDrive\\Oliver\\0_OM\\Training\\DeepReasoning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# the huggingface datasets for model training\n",
    "from datasets import load_dataset\n",
    "\n",
    "# our standard data exploration and wrangling packages\n",
    "import re\n",
    "import random\n",
    "import plotnine as p9\n",
    "import pandas   as pd\n",
    "import numpy    as np\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "# set working directory\n",
    "os.chdir('c:\\\\Users\\\\OliverMorris\\\\OneDrive\\\\Oliver\\\\0_OM\\\\Training\\\\DeepReasoning')\n",
    "path = os.getcwd()\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset wiki40b (C:\\Users\\OliverMorris\\.cache\\huggingface\\datasets\\wiki40b\\en\\1.1.0\\4efc066a938719e75ab321a6cd1c7b05006dcaf338071cdd92ab5652d3c453b8)\n",
      "dict_keys(['train', 'validation', 'test'])\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Length of training set: 2926536\n",
      "Length of validation set: 163597\n",
      "Length of testing set: 162274\n"
     ]
    }
   ],
   "source": [
    "# get the dataset \n",
    "dataset = load_dataset('wiki40b', 'en') #, split='test'\n",
    "\n",
    "print(dataset.keys())\n",
    "print(type(dataset['train']))\n",
    "\n",
    "print('Length of training set:',   len(dataset['train']))\n",
    "print('Length of validation set:', len(dataset['validation']))\n",
    "print('Length of testing set:',    len(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'wikidata_id': 'Q5004781',\n",
       " 'text': \"\\n_START_ARTICLE_\\nBácum Municipality\\n_START_SECTION_\\nYaqui Indians\\n_START_PARAGRAPH_\\nThe indigenous community is very large, with more than 9,000 residents identified as Yaqui in the 2000 census.\\n_START_SECTION_\\nEconomy\\n_START_PARAGRAPH_\\nThe Bácum Municipality's main economic activity is intensive agriculture, with more than 300 square kilometers under irrigation by canal.  The Yaqui River's water is used for irrigation in a system of canals. The main crops are wheat, corn, soybeans, barley, cotton, and garden vegetables, as well as seasonal crops such as alfalfa and some fruits. _NEWLINE_The coastline in the south is 12 kilometres (7.5\\xa0mi) long.  Some fishing is practiced.  Industries are small, and consist chiefly of packing houses for vegetables and liquid fertilizer production.\",\n",
       " 'version_id': '3944630243645935402'}"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "\n",
    "# Let's inspect some examples...\n",
    "\n",
    "# get 20 random samples of the data\n",
    "sample_list= random.sample(range(len(dataset['train'])), 20)\n",
    "\n",
    "small_dataset = dataset['train'].select(sample_list)\n",
    "\n",
    "# print the first example\n",
    "small_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract sentences which have the key words 'because' or 'therefore'.\n",
    "# the function then separates the sentences in the chunk before the key word and the chunk after.\n",
    "\n",
    "def get_sentences_with_word(the_dataset, searchword, cause_last=True):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    pat_a = re.compile(r\"[A-Z]{1}[^\\._!?]*\\b\" + searchword + r\"\\b\\s+[^.!?\\n]*\", re.M) # extract sentences which contain a keyword out of the larger text \n",
    "    pat_b = re.compile(r\".+?(?=\" + searchword + r\")\", re.M) # extract chunk of sentence before the searchword\n",
    "    pat_c = re.compile(r\"(?<=\" + searchword + r\").+\", re.M) # extract chunk of sentence after the keyword\n",
    "    \n",
    "    # patterns for separating the sentence into cause and effect\n",
    "    # eg for \"I love ice cream because its sweet\", cause is last, cause = \"its sweet\", effect=\"I love ice cream\"\n",
    "    # eg for \"its sweet therefore I love ice cream\", cause is first, cause = \"its sweet\", effect=\"I love ice cream\"\n",
    "    if cause_last:\n",
    "        pat_effect = pat_b\n",
    "        pat_cause  = pat_c\n",
    "    else:\n",
    "        pat_effect = pat_c\n",
    "        pat_cause  = pat_b\n",
    "    \n",
    "    # loop thru each text\n",
    "    for i in tq.tqdm(range(len(the_dataset))):\n",
    "\n",
    "        # get next record\n",
    "        text        = the_dataset[i]['text']\n",
    "        wikidata_id = the_dataset[i]['wikidata_id']\n",
    "\n",
    "        # remove periods (full stops) which aren't sentence endings\n",
    "        # this is tough, won't be perfect\n",
    "        text = text.replace(\"e.g.\", \"eg\")\n",
    "        text = text.replace(\"i.e.\", \"ie\")\n",
    "        \n",
    "        # replace any instance of the ASCII encodable separator we use for the pandas file, which is a tilde ~ (ASCII 126)\n",
    "        text = text.replace(\"~\", \"-\")\n",
    "\n",
    "        # remove any instance of the wiki40b tokens for end of sentence, replace with the fullstop marker \". \"\n",
    "        text = text.replace(\"\\n_START_SECTION_\\n\", \". \")\n",
    "        text = text.replace(\"\\n_START_PARAGRAPH_\\n\", \". \")\n",
    "\n",
    "        # extract sentences with the searchword, eg 'because'\n",
    "        try:\n",
    "            \n",
    "            found_a = pat_a.findall(text)\n",
    "            \n",
    "        except AttributeError:\n",
    "            found_a = ''\n",
    "    \n",
    "        # for each sentence with the searchword which was found within the text\n",
    "        for j in range(len(found_a)):\n",
    "\n",
    "            # separate the sentence into cause and effect chunks\n",
    "            cause  = pat_cause.findall(found_a[j])[0]\n",
    "            effect = pat_effect.findall(found_a[j])[0]\n",
    "\n",
    "            # both chunks must be longer than 4 words to bother saving them\n",
    "            if len(cause.split())>4 and len(effect.split())>4 :\n",
    "                # append the cause and effect sentence chunks to the list\n",
    "                sentences.append({'wikidata_id':wikidata_id, 'searchword':searchword, 'incidence':j, 'effect':effect, 'cause':cause})\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1114.08it/s]\n",
      "\n",
      "Source Data from Wiki40b\n",
      "\n",
      "\n",
      "{'wikidata_id': ['Q431031'], 'text': [\"\\n_START_ARTICLE_\\nVolunteer Point\\n_START_PARAGRAPH_\\nVolunteer Point is a headland on the east coast of East Falkland, in the Falkland Islands, north-northeast of Stanley, and east of Johnson's Harbour and Berkeley Sound._NEWLINE_It is at the end of a narrow peninsula, which protects Volunteer Lagoon. At its landward end is Volunteer Shanty, a well maintained outhouse, which was used by trekkers until a few years ago.\\n_START_SECTION_\\nStrategic value in Falklands War\\n_START_PARAGRAPH_\\nVolunteer Point is one of the easternmost points of the islands, but Cape Pembroke is the furthest east. During the Falklands War, Argentine commanders considered it a potential British landing point, because it was far from continental Argentine airbases (e.g. Rio Grande, Comodoro Rivadavia), and those at Pebble Island, but also a strategic foothold for any British force wishing to retake Stanley. However, in the event, the British landings took place on San Carlos Water in the west of East Falkland, on Falkland Sound.\\n_START_SECTION_\\nWildlife\\n_START_PARAGRAPH_\\nVolunteer Point has been identified by BirdLife International as an Important Bird Area (IBA).  Birds for which the site is of conservation significance include Falkland steamer ducks (75 breeding pairs), ruddy-headed geese (100 pairs), gentoo penguins (100 pairs), Magellanic penguins (2000 pairs) and white-bridled finches._NEWLINE_Volunteer Point is notable for having about 150 pairs of king penguins breed here, at the most northerly part of their range. King penguins were once nearly extinct in the Falklands, and Volunteer Point contains most of the Falkland population. There are also southern elephant seals.\"], 'version_id': ['16172487969528922053']}\n",
      "\n",
      "\n",
      "First Extracted Sentence\n",
      "\n",
      "\n",
      "Effect\n",
      "During the Falklands War, Argentine commanders considered it a potential British landing point, \n",
      "\n",
      "\n",
      "Cause\n",
      " it was far from continental Argentine airbases (eg Rio Grande, Comodoro Rivadavia), and those at Pebble Island, but also a strategic foothold for any British force wishing to retake Stanley\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the function on the short sample\n",
    "\n",
    "sample_ext = get_sentences_with_word(the_dataset=small_dataset, searchword='because', cause_last=True)\n",
    "sample_ext = pd.DataFrame(sample_ext)\n",
    "sample_ext.to_csv(path_or_buf=os.path.join(path,'sentences_sample.csv'), index=False, sep='¬')\n",
    "\n",
    "# Its important that we work from the version saved to file because thats what the final version will do\n",
    "# get first wikidata id\n",
    "sample_ext_id1 = sample_ext.iloc[0,]['wikidata_id']\n",
    "print('\\n')\n",
    "print('Source Data from Wiki40b')\n",
    "print('\\n')\n",
    "print( small_dataset[ [id=='Q431031' for id in small_dataset['wikidata_id']] ] )\n",
    "\n",
    "print('\\n')\n",
    "print('First Extracted Sentence')\n",
    "\n",
    "print('\\n')\n",
    "print('Effect')\n",
    "print(sample_ext.loc[0,'effect'])\n",
    "print('\\n')\n",
    "print('Cause')\n",
    "print(sample_ext.loc[0,'cause'])\n",
    "\n",
    "#print(pd.read_csv(os.path.join(path,'sentences_sample.csv'), sep='¬'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 249/2926536 [00:00<19:44, 2471.27it/s]\n",
      "\n",
      "Datasplit: train . Searchword: because \n",
      "\n",
      "100%|██████████| 2926536/2926536 [22:19<00:00, 2184.56it/s]\n",
      "  0%|          | 533/2926536 [00:00<18:02, 2702.75it/s]\n",
      "\n",
      "Datasplit: train . Searchword: therefore \n",
      "\n",
      "100%|██████████| 2926536/2926536 [21:28<00:00, 2271.69it/s]\n",
      "  0%|          | 194/162274 [00:00<01:24, 1927.04it/s]\n",
      "\n",
      "Datasplit: test . Searchword: because \n",
      "\n",
      "100%|██████████| 162274/162274 [01:08<00:00, 2385.32it/s]\n",
      "  0%|          | 219/162274 [00:00<01:24, 1917.61it/s]\n",
      "\n",
      "Datasplit: test . Searchword: therefore \n",
      "\n",
      "100%|██████████| 162274/162274 [01:04<00:00, 2508.32it/s]\n",
      "  0%|          | 533/163597 [00:00<01:00, 2679.11it/s]\n",
      "\n",
      "Datasplit: validation . Searchword: because \n",
      "\n",
      "100%|██████████| 163597/163597 [01:07<00:00, 2417.93it/s]\n",
      "  0%|          | 277/163597 [00:00<00:59, 2750.67it/s]\n",
      "\n",
      "Datasplit: validation . Searchword: therefore \n",
      "\n",
      "100%|██████████| 163597/163597 [01:04<00:00, 2516.92it/s]\n"
     ]
    }
   ],
   "source": [
    "searchwords = [{'searchword':'because',  'cause_last':'True'} , \n",
    "               {'searchword':'therefore','cause_last':'False'} ]\n",
    "\n",
    "datasplits  = ['train', 'test', 'validation']\n",
    "\n",
    "for split in datasplits:\n",
    "\n",
    "    causes_and_effects = []\n",
    "\n",
    "    # get instances of causes and effects from wikipedia entries\n",
    "    for pair in searchwords:\n",
    "        print('\\n')\n",
    "        print(\"Datasplit:\",split,\". Searchword:\", pair['searchword'], '\\n')\n",
    "        \n",
    "        causes_and_effects = causes_and_effects.append(get_sentences_with_word( the_dataset= dataset[split],\n",
    "                                                                                searchword = pair['searchword'], \n",
    "                                                                                cause_last = eval(pair['cause_last'])))\n",
    "\n",
    "    # convert to pandas\n",
    "    causes_and_effects = pd.DataFrame(causes_and_effects)\n",
    "\n",
    "    # simply remove non ASCII characters, because these cannot be encoded by BERT\n",
    "    causes_and_effects['cause']  = causes_and_effects['cause'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "    causes_and_effects['effect'] = causes_and_effects['effect'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # save to file\n",
    "    causes_and_effects.to_csv(path_or_buf=os.path.join(path,'sentences_'+split+'.csv'), index=False, sep='~', encoding='ascii')\n",
    "\n",
    "#del causes_and_effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      wikidata_id searchword  incidence  \\\n",
       "25625   Q18344462    because          0   \n",
       "11633   Q65122069    because          5   \n",
       "28754   Q22905833  therefore          3   \n",
       "\n",
       "                                                  effect  \\\n",
       "25625  When they look for rare things (such as a jell...   \n",
       "11633  The actual effectiveness of wargaming in this ...   \n",
       "28754   lower compensation of the suppliers) worsen i...   \n",
       "\n",
       "                                                   cause  \n",
       "25625   the probability of success and the stakes are...  \n",
       "11633   officers use many tools to hone their decisio...  \n",
       "28754  If consumers tend to be higher income persons ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wikidata_id</th>\n      <th>searchword</th>\n      <th>incidence</th>\n      <th>effect</th>\n      <th>cause</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25625</th>\n      <td>Q18344462</td>\n      <td>because</td>\n      <td>0</td>\n      <td>When they look for rare things (such as a jell...</td>\n      <td>the probability of success and the stakes are...</td>\n    </tr>\n    <tr>\n      <th>11633</th>\n      <td>Q65122069</td>\n      <td>because</td>\n      <td>5</td>\n      <td>The actual effectiveness of wargaming in this ...</td>\n      <td>officers use many tools to hone their decisio...</td>\n    </tr>\n    <tr>\n      <th>28754</th>\n      <td>Q22905833</td>\n      <td>therefore</td>\n      <td>3</td>\n      <td>lower compensation of the suppliers) worsen i...</td>\n      <td>If consumers tend to be higher income persons ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "# let's look at some results from memory\n",
    "causes_and_effects.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       wikidata_id searchword  incidence  \\\n",
       "226283    Q2349884    because          0   \n",
       "200028     Q462058    because          1   \n",
       "439190    Q4931374    because          0   \n",
       "\n",
       "                                                   effect  \\\n",
       "226283  The price of these ships was 9 million and is ...   \n",
       "200028  The guaranteed lending program was eliminated ...   \n",
       "439190  From 1841 until 1872 Somerville was run by the...   \n",
       "\n",
       "                                                    cause  \n",
       "226283   it was a part of the offset deal for the prev...  \n",
       "200028   of a widespread perception that the governmen...  \n",
       "439190   up to that point Somerville was still incorpo...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wikidata_id</th>\n      <th>searchword</th>\n      <th>incidence</th>\n      <th>effect</th>\n      <th>cause</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>226283</th>\n      <td>Q2349884</td>\n      <td>because</td>\n      <td>0</td>\n      <td>The price of these ships was 9 million and is ...</td>\n      <td>it was a part of the offset deal for the prev...</td>\n    </tr>\n    <tr>\n      <th>200028</th>\n      <td>Q462058</td>\n      <td>because</td>\n      <td>1</td>\n      <td>The guaranteed lending program was eliminated ...</td>\n      <td>of a widespread perception that the governmen...</td>\n    </tr>\n    <tr>\n      <th>439190</th>\n      <td>Q4931374</td>\n      <td>because</td>\n      <td>0</td>\n      <td>From 1841 until 1872 Somerville was run by the...</td>\n      <td>up to that point Somerville was still incorpo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Let's also inspect the data we saved to file in the previous chunk\n",
    "# Keen to prove there are no file encoding problems\n",
    "causes_and_effects_sv = pd.read_csv(os.path.join(path,'sentences_train.csv'), sep='~', encoding='ascii' )\n",
    "\n",
    "# get 3 random samples of the data\n",
    "causes_and_effects_sv.sample(n=3)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Data Limitations\n",
    "\n",
    "The above examples show how easily we can extract huge numbers of examples with which to train our model. It also shows the pitfalls of the data. We can easily imagine a human sentence saying \"The chicken crossed the road because, well, it just did\". Our model won't learn anything about causation in the real world from this or the thousands of other incomplete justifications to be found on wikipedia. But this is not the point, the model is intentionally being taught plausible justifications, which is certainly a currency in which humans trade. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using BERT to get Embeddings from Text\n",
    "\n",
    "Transformer models will happily accept the integer indexes of tokens and learn their own embeddings. However, Transformers are also slow to train. There may be advantage in presenting the transformer with embeddings from BERT, rather than expect it to learn them.\n",
    "\n",
    "We'll use the huggingface BERT transformer and associated tokenizer to process the text into embedding vectors which we can then pass into our own transformer.\n",
    "https://huggingface.co/transformers/model_doc/bert.html\n",
    "\n",
    "The following code borrows from the Torch example at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "# this is the huggingface BertTokenizer: https://huggingface.co/transformers/model_doc/bert.html\n",
    "# it has a vocabulary of 30,522 tokens: https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "source": [
    "## Encoding Text in Preparation for Tokenisation\n",
    "\n",
    "BERT can take as input either one or two sentences, and uses the special token SEP to differentiate them. The CLS token always appears at the start of the text, and is specific to classification tasks. The tokenzier ensures these special tokens are present\n",
    "\n",
    "Both tokens are always required, however, even if we only have one sentence, and even if we are not using BERT for classification. That’s how BERT was pre-trained, and so that’s what BERT expects to see.\n",
    "\n",
    "- 2 Sentence Input:\n",
    "\n",
    "    - [[CLS]] The man went to the store. [[SEP]] He bought a gallon of milk.\n",
    "\n",
    "- 1 Sentence Input:\n",
    "\n",
    "    - [[CLS]] The man went to the store. [[SEP]]\n",
    "\n",
    "We must encode the 'cause' and 'effect' clauses into the format BERT expects. We use this method to encode:\n",
    "\n",
    "**tokenizer.encode_plustokenizer.encode_plus**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# here's an example\n",
    "clause = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + clause + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ]
  },
  {
   "source": [
    "### The Tokenizer's Output\n",
    "\n",
    "Notice how the word “embeddings” is represented:\n",
    "\n",
    "'em', '##bed', '##ding', '##s'\n",
    "\n",
    "The original word has been split into smaller subwords and characters. The two hash signs preceding some of these subwords are just our tokenizer’s way to denote that this subword or character is part of a larger word and preceded by another subword. So, for example, the ‘##bed’ token is separate from the ‘bed’ token; the first is used whenever the subword ‘bed’ occurs within a larger word and the second is used explicitly for when the standalone token ‘thing you sleep on’ occurs.\n",
    "\n",
    "Why does it look this way? This is because the BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data. Since the vocabulary limit size of our BERT tokenizer model is 30,000, the WordPiece model generated a vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on. This vocabulary contains four things:\n",
    "\n",
    "- Whole words\n",
    "- Subwords occuring at the front of a word or in isolation (“em” as in “embeddings” is assigned the same vector as the standalone sequence of characters “em” as in “go get em” )\n",
    "- Subwords not at the front of a word, which are preceded by ‘##’ to denote this case\n",
    "- Individual characters\n",
    "To tokenize a word under this model, the tokenizer first checks if the whole word is in the vocabulary. If not, it tries to break the word into the largest possible subwords contained in the vocabulary, and as a last resort will decompose the word into individual characters. Note that because of this, we can always represent a word as, at the very least, the collection of its individual characters.\n",
    "\n",
    "As a result, rather than assigning out of vocabulary words to a catch-all token like ‘OOV’ or ‘UNK’, words that are not in the vocabulary are decomposed into subword and character tokens that we can then generate embeddings for.\n",
    "\n",
    "So, rather than assigning “embeddings” and every other out of vocabulary word to an overloaded unknown vocabulary token, we split it into subword tokens ‘em’, ‘##bed’, ‘##ding’, ‘##s’ that will retain some of the contextual meaning of the original word. We can even average these subword embedding vectors to generate an approximate vector for the original word."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "def get_indexed_tokens(text):\n",
    "\n",
    "    # Add the special tokens.\n",
    "    marked_text = '[CLS] ' + text  + ' [SEP]'\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    try:\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    except:\n",
    "        tokenized_text = ['[CLS]','[SEP]']\n",
    "\n",
    "    # sometimes more than 512 tokens result, the model exceeds its limits\n",
    "    # This throws an error which we handle by simply returning the start and end tokens\n",
    "    # Later these clauses will be removed from the pandas tables of train/test/validate data\n",
    "    if len(tokenized_text) > 512:\n",
    "        tokenized_text = ['[CLS]','[SEP]']\n",
    "\n",
    "    # Map the token strings to their vocabulary indices.\n",
    "    indexes_of_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    return indexes_of_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing:  sentences_train.csv , clause:  cause \n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=516382.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9b113a2dd914c2a86506b80d8cff147"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing:  sentences_train.csv , clause:  effect \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=516382.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14a045b8378e43e1b9d01e1a1db65891"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing:  sentences_test.csv , clause:  cause \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=28665.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64fcbe29e44942549fffe256ee7f5a73"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing:  sentences_test.csv , clause:  effect \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=28665.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "906deaa7ddea4c3eb4369302463055d1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing:  sentences_validation.csv , clause:  cause \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=28940.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc407d843838492daa42dc83d0728e42"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing:  sentences_validation.csv , clause:  effect \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=28940.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "693d253d226f4df3950a79feb9ec4b9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# tokenise both 'cause' and 'effect' clauses for all datasets; training, testing and validation.\n",
    "for filename in ['sentences_train.csv', 'sentences_test.csv', 'sentences_validation.csv']:\n",
    "\n",
    "    # load file of saved clauses\n",
    "    causes_and_effects_sv = pd.read_csv(os.path.join(path, filename), sep='~')\n",
    "\n",
    "    # setup parallel processing\n",
    "    n_cpus = os.cpu_count()\n",
    "    executor = ThreadPoolExecutor(max_workers=n_cpus)\n",
    "\n",
    "    # for both cause and effect clauses...\n",
    "    for clause in ['cause', 'effect']:\n",
    "\n",
    "        # tell us whats happening!\n",
    "        print('Processing: ', filename, ', clause: ', clause, '\\n')\n",
    "\n",
    "        # setup blank list to receive results\n",
    "        tokenized_idxs = []\n",
    "\n",
    "        # setup list comprehension to process test into indexed tokens using parallelism\n",
    "        indexed_tokens = [executor.submit(get_indexed_tokens, text) for text in tq.tqdm(causes_and_effects_sv[clause])]\n",
    "\n",
    "        # yield parallel results and append to blank list\n",
    "        for indexed_token in as_completed(indexed_tokens):\n",
    "            tokenized_idxs.append(indexed_token.result())\n",
    "    \n",
    "        # record full results in new column in pandas dataframe\n",
    "        causes_and_effects_sv[ clause + '_idxs' ] = tokenized_idxs\n",
    "\n",
    "    # save pandas dataframe to file\n",
    "    causes_and_effects_sv.to_csv(path_or_buf=os.path.join(path, filename), index=False, sep='~')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  wikidata_id searchword  incidence  \\\n",
       "0   Q18200409    because          0   \n",
       "1    Q4771251    because          0   \n",
       "2    Q1903279    because          0   \n",
       "3   Q15197036    because          0   \n",
       "4    Q4882660    because          0   \n",
       "\n",
       "                                              effect  \\\n",
       "0  At school, Harry proclaims to Conor that he wi...   \n",
       "1  This leads to the somewhat surprising observat...   \n",
       "2  It is a variation of the crows foot style of d...   \n",
       "3  Based on surveys conducted in 1997 and 1998, t...   \n",
       "4  They were postponed to September just a day be...   \n",
       "\n",
       "                                               cause  \\\n",
       "0                            he no longer sees [him]   \n",
       "1   as the oxide grows thinner, the leakage goes ...   \n",
       "2   of its readability and efficient use of drawi...   \n",
       "3   it supports small numbers of breeding endange...   \n",
       "4   Kosovo's representative wanted to have Kosovo...   \n",
       "\n",
       "                                          cause_idxs  \\\n",
       "0  [101, 2057, 2856, 1996, 2143, 2302, 4209, 2151...   \n",
       "1  [101, 2049, 5915, 3930, 2144, 1996, 4134, 2018...   \n",
       "2  [101, 1997, 1037, 5164, 1997, 6181, 1998, 2316...   \n",
       "3  [101, 4897, 2063, 1005, 1055, 9872, 2003, 6827...   \n",
       "4  [101, 1997, 1996, 16376, 4180, 1997, 4487, 649...   \n",
       "\n",
       "                                         effect_idxs  \n",
       "0  [101, 4998, 2013, 2023, 1010, 17981, 2036, 169...  \n",
       "1  [101, 1996, 2277, 1997, 2702, 2319, 1010, 2328...  \n",
       "2  [101, 1996, 2197, 2343, 28709, 12942, 4900, 20...  \n",
       "3  [101, 2065, 2136, 2372, 2907, 2067, 1998, 2024...  \n",
       "4  [101, 1996, 3732, 2003, 6887, 2891, 8458, 1025...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wikidata_id</th>\n      <th>searchword</th>\n      <th>incidence</th>\n      <th>effect</th>\n      <th>cause</th>\n      <th>cause_idxs</th>\n      <th>effect_idxs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q18200409</td>\n      <td>because</td>\n      <td>0</td>\n      <td>At school, Harry proclaims to Conor that he wi...</td>\n      <td>he no longer sees [him]</td>\n      <td>[101, 2057, 2856, 1996, 2143, 2302, 4209, 2151...</td>\n      <td>[101, 4998, 2013, 2023, 1010, 17981, 2036, 169...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q4771251</td>\n      <td>because</td>\n      <td>0</td>\n      <td>This leads to the somewhat surprising observat...</td>\n      <td>as the oxide grows thinner, the leakage goes ...</td>\n      <td>[101, 2049, 5915, 3930, 2144, 1996, 4134, 2018...</td>\n      <td>[101, 1996, 2277, 1997, 2702, 2319, 1010, 2328...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q1903279</td>\n      <td>because</td>\n      <td>0</td>\n      <td>It is a variation of the crows foot style of d...</td>\n      <td>of its readability and efficient use of drawi...</td>\n      <td>[101, 1997, 1037, 5164, 1997, 6181, 1998, 2316...</td>\n      <td>[101, 1996, 2197, 2343, 28709, 12942, 4900, 20...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q15197036</td>\n      <td>because</td>\n      <td>0</td>\n      <td>Based on surveys conducted in 1997 and 1998, t...</td>\n      <td>it supports small numbers of breeding endange...</td>\n      <td>[101, 4897, 2063, 1005, 1055, 9872, 2003, 6827...</td>\n      <td>[101, 2065, 2136, 2372, 2907, 2067, 1998, 2024...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q4882660</td>\n      <td>because</td>\n      <td>0</td>\n      <td>They were postponed to September just a day be...</td>\n      <td>Kosovo's representative wanted to have Kosovo...</td>\n      <td>[101, 1997, 1996, 16376, 4180, 1997, 4487, 649...</td>\n      <td>[101, 1996, 3732, 2003, 6887, 2891, 8458, 1025...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# let's look at what we made...\n",
    "causes_and_effects_sv.head()\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Sentence 0 and Sentence 1\n",
    "\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, sentence 0 will be the cause, sentence 1 will be the effect.\n",
    "\n",
    "Next we need to convert our data to torch tensors and call the BERT model. \n",
    "\n",
    "The BERT PyTorch interface requires that the data be in torch tensors rather than Python lists, so we convert the lists here - this does not change the shape or the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "How many records do we have?\ntrain  :  516382\ntesti  :  28665\nvalid  :  28940\n"
     ]
    }
   ],
   "source": [
    "# Let's load our data from file\n",
    "from ast import literal_eval\n",
    "\n",
    "def get_pandasfromcsv(filename):\n",
    "\n",
    "    data = pd.read_csv(os.path.join(path,filename), sep='~')\n",
    "\n",
    "    # The Pandas to_csv() function stores lists as strings, eg [1,2] becomes '[1,2]', this is because list objects are not native to the cells of pandas dataframes\n",
    "    # we could have pickled the pandas object, but easier to simply 'evaluate' the string literal into a list. \n",
    "    data['cause_idxs' ] = data['cause_idxs'].apply(literal_eval)\n",
    "    data['effect_idxs'] = data['effect_idxs'].apply(literal_eval)\n",
    "\n",
    "    return data\n",
    "\n",
    "train = get_pandasfromcsv('sentences_train.csv')\n",
    "train.name = 'train'\n",
    "testi = get_pandasfromcsv('sentences_test.csv')\n",
    "testi.name = 'testi'\n",
    "valid = get_pandasfromcsv('sentences_validation.csv')\n",
    "valid.name = 'valid'\n",
    "\n",
    "print('How many records do we have?')\n",
    "print(train.name,' : ', len(train))\n",
    "print(testi.name,' : ', len(testi))\n",
    "print(valid.name,' : ', len(valid))\n"
   ]
  },
  {
   "source": [
    "### Clause Length\n",
    "\n",
    "Some clauses are very long and some very short. All clauses will need to be padded to the same length for submitting to the model. So if we simply pad all clauses to the longest possible, then we will have huge tensors of mostly zeros (padding). Also, some clauses are very short and contain effectively no information from which the model can learn. It will be btter to crop the unsually long and unsually short clauses. \n",
    "\n",
    "To decide what max length to use we'll need to see the distribution of lengths, then aim for something like the 90th percentile.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cause Sentences...\nThe longest sentence has a token length of:  436\nThe 80th percentile of sentences have token length of:  23.0\n\n\nEffect Sentences...\nThe longest sentence has a token length of:  414\nThe 80th percentile of sentences have token length of:  23.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"323.630625pt\" version=\"1.1\" viewBox=\"0 0 425.109937 323.630625\" width=\"425.109937pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-01T23:34:20.634818</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 323.630625 \r\nL 425.109937 323.630625 \r\nL 425.109937 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 60.789938 287.612 \r\nL 236.829938 287.612 \r\nL 236.829938 21.5 \r\nL 60.789938 21.5 \r\nz\r\n\" style=\"fill:#ebebeb;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 70.569938 287.612 \r\nL 70.569938 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <path d=\"M 70.569938 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 2.75 \r\n\" id=\"mfa31e7fa65\" style=\"stroke:#000000;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"70.569938\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(67.770438 299.248625)scale(0.088 -0.088)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 106.133574 287.612 \r\nL 106.133574 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <path d=\"M 106.133574 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"106.133574\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(97.735074 299.248625)scale(0.088 -0.088)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 141.69721 287.612 \r\nL 141.69721 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <path d=\"M 141.69721 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"141.69721\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(133.29871 299.248625)scale(0.088 -0.088)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 177.260847 287.612 \r\nL 177.260847 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <path d=\"M 177.260847 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"177.260847\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(168.862347 299.248625)scale(0.088 -0.088)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 212.824483 287.612 \r\nL 212.824483 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <path d=\"M 212.824483 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"212.824483\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(204.425983 299.248625)scale(0.088 -0.088)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 88.351756 287.612 \r\nL 88.351756 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_12\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 123.915392 287.612 \r\nL 123.915392 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 159.479028 287.612 \r\nL 159.479028 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_14\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 195.042665 287.612 \r\nL 195.042665 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 230.606301 287.612 \r\nL 230.606301 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_16\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 275.516 \r\nL 236.829938 275.516 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_17\">\r\n      <path d=\"M 60.789938 275.516 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -2.75 0 \r\n\" id=\"m28ee886540\" style=\"stroke:#000000;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"60.789938\" xlink:href=\"#m28ee886540\" y=\"275.516\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(50.240938 277.94425)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_18\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 226.41712 \r\nL 236.829938 226.41712 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_19\">\r\n      <path d=\"M 60.789938 226.41712 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"60.789938\" xlink:href=\"#m28ee886540\" y=\"226.41712\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 50000 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(27.844938 228.84537)scale(0.088 -0.088)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_20\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 177.318241 \r\nL 236.829938 177.318241 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_21\">\r\n      <path d=\"M 60.789938 177.318241 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"60.789938\" xlink:href=\"#m28ee886540\" y=\"177.318241\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 100000 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(22.245938 179.746491)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_22\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 128.219361 \r\nL 236.829938 128.219361 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_23\">\r\n      <path d=\"M 60.789938 128.219361 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"60.789938\" xlink:href=\"#m28ee886540\" y=\"128.219361\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 150000 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(22.245938 130.647611)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_24\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 79.120481 \r\nL 236.829938 79.120481 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_25\">\r\n      <path d=\"M 60.789938 79.120481 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"60.789938\" xlink:href=\"#m28ee886540\" y=\"79.120481\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 200000 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(22.245938 81.548731)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_26\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 30.021602 \r\nL 236.829938 30.021602 \r\n\" style=\"fill:none;stroke:#ffffff;\"/>\r\n     </g>\r\n     <g id=\"line2d_27\">\r\n      <path d=\"M 60.789938 30.021602 \r\n\" style=\"fill:none;stroke:#333333;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"60.789938\" xlink:href=\"#m28ee886540\" y=\"30.021602\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 250000 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(22.245938 32.449852)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_28\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 250.96656 \r\nL 236.829938 250.96656 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_29\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 201.86768 \r\nL 236.829938 201.86768 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_30\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 152.768801 \r\nL 236.829938 152.768801 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_10\">\r\n     <g id=\"line2d_31\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 103.669921 \r\nL 236.829938 103.669921 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_11\">\r\n     <g id=\"line2d_32\">\r\n      <path clip-path=\"url(#pc766597775)\" d=\"M 60.789938 54.571041 \r\nL 236.829938 54.571041 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_12\">\r\n     <!-- count -->\r\n     <g transform=\"translate(15.558281 168.344875)rotate(-90)scale(0.11 -0.11)\">\r\n      <defs>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"116.162109\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"179.541016\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"242.919922\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"PolyCollection_1\">\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 68.791756 275.516 \r\nL 68.791756 275.514036 \r\nL 72.348119 275.514036 \r\nL 72.348119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 72.348119 275.516 \r\nL 72.348119 33.596 \r\nL 75.904483 33.596 \r\nL 75.904483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 75.904483 275.516 \r\nL 75.904483 92.475377 \r\nL 79.460847 92.475377 \r\nL 79.460847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 79.460847 275.516 \r\nL 79.460847 193.49534 \r\nL 83.01721 193.49534 \r\nL 83.01721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 83.01721 275.516 \r\nL 83.01721 242.432193 \r\nL 86.573574 242.432193 \r\nL 86.573574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 86.573574 275.516 \r\nL 86.573574 262.38696 \r\nL 90.129938 262.38696 \r\nL 90.129938 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 90.129938 275.516 \r\nL 90.129938 270.121997 \r\nL 93.686301 270.121997 \r\nL 93.686301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 93.686301 275.516 \r\nL 93.686301 273.15729 \r\nL 97.242665 273.15729 \r\nL 97.242665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 97.242665 275.516 \r\nL 97.242665 274.345483 \r\nL 100.799028 274.345483 \r\nL 100.799028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 100.799028 275.516 \r\nL 100.799028 274.913066 \r\nL 104.355392 274.913066 \r\nL 104.355392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 104.355392 275.516 \r\nL 104.355392 275.171326 \r\nL 107.911756 275.171326 \r\nL 107.911756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 107.911756 275.516 \r\nL 107.911756 275.314695 \r\nL 111.468119 275.314695 \r\nL 111.468119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 111.468119 275.516 \r\nL 111.468119 275.410928 \r\nL 115.024483 275.410928 \r\nL 115.024483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 115.024483 275.516 \r\nL 115.024483 275.447262 \r\nL 118.580847 275.447262 \r\nL 118.580847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 118.580847 275.516 \r\nL 118.580847 275.461009 \r\nL 122.13721 275.461009 \r\nL 122.13721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 122.13721 275.516 \r\nL 122.13721 275.487523 \r\nL 125.693574 275.487523 \r\nL 125.693574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 125.693574 275.516 \r\nL 125.693574 275.49636 \r\nL 129.249938 275.49636 \r\nL 129.249938 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 129.249938 275.516 \r\nL 129.249938 275.497342 \r\nL 132.806301 275.497342 \r\nL 132.806301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 132.806301 275.516 \r\nL 132.806301 275.497342 \r\nL 136.362665 275.497342 \r\nL 136.362665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 136.362665 275.516 \r\nL 136.362665 275.504216 \r\nL 139.919028 275.504216 \r\nL 139.919028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 139.919028 275.516 \r\nL 139.919028 275.505198 \r\nL 143.475392 275.505198 \r\nL 143.475392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 143.475392 275.516 \r\nL 143.475392 275.512072 \r\nL 147.031756 275.512072 \r\nL 147.031756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 147.031756 275.516 \r\nL 147.031756 275.51109 \r\nL 150.588119 275.51109 \r\nL 150.588119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 150.588119 275.516 \r\nL 150.588119 275.512072 \r\nL 154.144483 275.512072 \r\nL 154.144483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 154.144483 275.516 \r\nL 154.144483 275.513054 \r\nL 157.700847 275.513054 \r\nL 157.700847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 157.700847 275.516 \r\nL 157.700847 275.512072 \r\nL 161.25721 275.512072 \r\nL 161.25721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 161.25721 275.516 \r\nL 161.25721 275.514036 \r\nL 164.813574 275.514036 \r\nL 164.813574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 164.813574 275.516 \r\nL 164.813574 275.514036 \r\nL 168.369938 275.514036 \r\nL 168.369938 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 168.369938 275.516 \r\nL 168.369938 275.515018 \r\nL 171.926301 275.515018 \r\nL 171.926301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 171.926301 275.516 \r\nL 171.926301 275.514036 \r\nL 175.482665 275.514036 \r\nL 175.482665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 175.482665 275.516 \r\nL 175.482665 275.515018 \r\nL 179.039028 275.515018 \r\nL 179.039028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 179.039028 275.516 \r\nL 179.039028 275.514036 \r\nL 182.595392 275.514036 \r\nL 182.595392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 182.595392 275.516 \r\nL 182.595392 275.516 \r\nL 186.151756 275.516 \r\nL 186.151756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 186.151756 275.516 \r\nL 186.151756 275.516 \r\nL 189.708119 275.516 \r\nL 189.708119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 189.708119 275.516 \r\nL 189.708119 275.515018 \r\nL 193.264483 275.515018 \r\nL 193.264483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 193.264483 275.516 \r\nL 193.264483 275.515018 \r\nL 196.820847 275.515018 \r\nL 196.820847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 196.820847 275.516 \r\nL 196.820847 275.516 \r\nL 200.37721 275.516 \r\nL 200.37721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 200.37721 275.516 \r\nL 200.37721 275.516 \r\nL 203.933574 275.516 \r\nL 203.933574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 203.933574 275.516 \r\nL 203.933574 275.516 \r\nL 207.489938 275.516 \r\nL 207.489938 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 207.489938 275.516 \r\nL 207.489938 275.514036 \r\nL 211.046301 275.514036 \r\nL 211.046301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 211.046301 275.516 \r\nL 211.046301 275.515018 \r\nL 214.602665 275.515018 \r\nL 214.602665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 214.602665 275.516 \r\nL 214.602665 275.514036 \r\nL 218.159028 275.514036 \r\nL 218.159028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 218.159028 275.516 \r\nL 218.159028 275.516 \r\nL 221.715392 275.516 \r\nL 221.715392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 221.715392 275.516 \r\nL 221.715392 275.516 \r\nL 225.271756 275.516 \r\nL 225.271756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pc766597775)\" d=\"M 225.271756 275.516 \r\nL 225.271756 275.515018 \r\nL 228.828119 275.515018 \r\nL 228.828119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 60.789938 21.5 \r\nL 236.829938 21.5 \r\nL 236.829938 7.2 \r\nL 60.789938 7.2 \r\nz\r\n\" style=\"fill:#d9d9d9;\"/>\r\n   </g>\r\n   <g id=\"text_13\">\r\n    <!-- cause -->\r\n    <g style=\"fill:#1a1a1a;\" transform=\"translate(135.90625 16.77825)scale(0.088 -0.088)\">\r\n     <defs>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"116.259766\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"179.638672\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"231.738281\" xlink:href=\"#DejaVuSans-101\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 241.869938 287.612 \r\nL 417.909937 287.612 \r\nL 417.909937 21.5 \r\nL 241.869938 21.5 \r\nz\r\n\" style=\"fill:#ebebeb;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_3\">\r\n    <g id=\"xtick_11\">\r\n     <g id=\"line2d_33\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 251.649938 287.612 \r\nL 251.649938 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n     <g id=\"line2d_34\">\r\n      <path d=\"M 251.649938 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-linecap:square;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"251.649938\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(248.850438 299.248625)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_12\">\r\n     <g id=\"line2d_35\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 287.213574 287.612 \r\nL 287.213574 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n     <g id=\"line2d_36\">\r\n      <path d=\"M 287.213574 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-linecap:square;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"287.213574\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 100 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(278.815074 299.248625)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_13\">\r\n     <g id=\"line2d_37\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 322.77721 287.612 \r\nL 322.77721 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n     <g id=\"line2d_38\">\r\n      <path d=\"M 322.77721 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-linecap:square;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"322.77721\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 200 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(314.37871 299.248625)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_14\">\r\n     <g id=\"line2d_39\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 358.340847 287.612 \r\nL 358.340847 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n     <g id=\"line2d_40\">\r\n      <path d=\"M 358.340847 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-linecap:square;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"358.340847\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 300 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(349.942347 299.248625)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_15\">\r\n     <g id=\"line2d_41\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 393.904483 287.612 \r\nL 393.904483 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n     <g id=\"line2d_42\">\r\n      <path d=\"M 393.904483 287.612 \r\n\" style=\"fill:none;stroke:#333333;stroke-linecap:square;stroke-width:1.5;\"/>\r\n      <g>\r\n       <use style=\"fill:#333333;stroke:#000000;\" x=\"393.904483\" xlink:href=\"#mfa31e7fa65\" y=\"287.612\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- 400 -->\r\n      <g style=\"fill:#4d4d4d;\" transform=\"translate(385.505983 299.248625)scale(0.088 -0.088)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_16\">\r\n     <g id=\"line2d_43\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 269.431756 287.612 \r\nL 269.431756 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_17\">\r\n     <g id=\"line2d_44\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 304.995392 287.612 \r\nL 304.995392 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_18\">\r\n     <g id=\"line2d_45\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 340.559028 287.612 \r\nL 340.559028 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_19\">\r\n     <g id=\"line2d_46\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 376.122665 287.612 \r\nL 376.122665 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_20\">\r\n     <g id=\"line2d_47\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 411.686301 287.612 \r\nL 411.686301 21.5 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_19\">\r\n     <!-- token_lengths -->\r\n     <g transform=\"translate(195.066734 313.837031)scale(0.11 -0.11)\">\r\n      <defs>\r\n       <path d=\"M 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 31.109375 \r\nL 44.921875 54.6875 \r\nL 56.390625 54.6875 \r\nL 27.390625 29.109375 \r\nL 57.625 0 \r\nL 45.90625 0 \r\nL 18.109375 26.703125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nz\r\n\" id=\"DejaVuSans-107\"/>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"100.390625\" xlink:href=\"#DejaVuSans-107\"/>\r\n      <use x=\"154.675781\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"216.199219\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"279.578125\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"329.578125\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"357.361328\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"418.884766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"482.263672\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"545.740234\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"584.949219\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"648.328125\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_4\">\r\n    <g id=\"ytick_12\">\r\n     <g id=\"line2d_48\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 275.516 \r\nL 417.909937 275.516 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_13\">\r\n     <g id=\"line2d_49\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 226.41712 \r\nL 417.909937 226.41712 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_14\">\r\n     <g id=\"line2d_50\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 177.318241 \r\nL 417.909937 177.318241 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_15\">\r\n     <g id=\"line2d_51\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 128.219361 \r\nL 417.909937 128.219361 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_16\">\r\n     <g id=\"line2d_52\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 79.120481 \r\nL 417.909937 79.120481 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_17\">\r\n     <g id=\"line2d_53\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 30.021602 \r\nL 417.909937 30.021602 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_18\">\r\n     <g id=\"line2d_54\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 250.96656 \r\nL 417.909937 250.96656 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_19\">\r\n     <g id=\"line2d_55\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 201.86768 \r\nL 417.909937 201.86768 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_20\">\r\n     <g id=\"line2d_56\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 152.768801 \r\nL 417.909937 152.768801 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_21\">\r\n     <g id=\"line2d_57\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 103.669921 \r\nL 417.909937 103.669921 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_22\">\r\n     <g id=\"line2d_58\">\r\n      <path clip-path=\"url(#pd2c6188e82)\" d=\"M 241.869938 54.571041 \r\nL 417.909937 54.571041 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.5;\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"PolyCollection_2\">\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 249.871756 275.516 \r\nL 249.871756 275.514036 \r\nL 253.428119 275.514036 \r\nL 253.428119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 253.428119 275.516 \r\nL 253.428119 35.449974 \r\nL 256.984483 35.449974 \r\nL 256.984483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 256.984483 275.516 \r\nL 256.984483 86.883014 \r\nL 260.540847 86.883014 \r\nL 260.540847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 260.540847 275.516 \r\nL 260.540847 193.951959 \r\nL 264.09721 193.951959 \r\nL 264.09721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 264.09721 275.516 \r\nL 264.09721 243.477999 \r\nL 267.653574 243.477999 \r\nL 267.653574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 267.653574 275.516 \r\nL 267.653574 263.233424 \r\nL 271.209938 263.233424 \r\nL 271.209938 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 271.209938 275.516 \r\nL 271.209938 270.553085 \r\nL 274.766301 270.553085 \r\nL 274.766301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 274.766301 275.516 \r\nL 274.766301 273.397874 \r\nL 278.322665 273.397874 \r\nL 278.322665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 278.322665 275.516 \r\nL 278.322665 274.594905 \r\nL 281.879028 274.594905 \r\nL 281.879028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 281.879028 275.516 \r\nL 281.879028 275.073128 \r\nL 285.435392 275.073128 \r\nL 285.435392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 285.435392 275.516 \r\nL 285.435392 275.26756 \r\nL 288.991756 275.26756 \r\nL 288.991756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 288.991756 275.516 \r\nL 288.991756 275.379505 \r\nL 292.548119 275.379505 \r\nL 292.548119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 292.548119 275.516 \r\nL 292.548119 275.456099 \r\nL 296.104483 275.456099 \r\nL 296.104483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 296.104483 275.516 \r\nL 296.104483 275.476721 \r\nL 299.660847 275.476721 \r\nL 299.660847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 299.660847 275.516 \r\nL 299.660847 275.484577 \r\nL 303.21721 275.484577 \r\nL 303.21721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 303.21721 275.516 \r\nL 303.21721 275.492433 \r\nL 306.773574 275.492433 \r\nL 306.773574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 306.773574 275.516 \r\nL 306.773574 275.492433 \r\nL 310.329938 275.492433 \r\nL 310.329938 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 310.329938 275.516 \r\nL 310.329938 275.508144 \r\nL 313.886301 275.508144 \r\nL 313.886301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 313.886301 275.516 \r\nL 313.886301 275.51109 \r\nL 317.442665 275.51109 \r\nL 317.442665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 317.442665 275.516 \r\nL 317.442665 275.51109 \r\nL 320.999028 275.51109 \r\nL 320.999028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 320.999028 275.516 \r\nL 320.999028 275.512072 \r\nL 324.555392 275.512072 \r\nL 324.555392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 324.555392 275.516 \r\nL 324.555392 275.51109 \r\nL 328.111756 275.51109 \r\nL 328.111756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 328.111756 275.516 \r\nL 328.111756 275.512072 \r\nL 331.668119 275.512072 \r\nL 331.668119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 331.668119 275.516 \r\nL 331.668119 275.514036 \r\nL 335.224483 275.514036 \r\nL 335.224483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 335.224483 275.516 \r\nL 335.224483 275.513054 \r\nL 338.780847 275.513054 \r\nL 338.780847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 338.780847 275.516 \r\nL 338.780847 275.514036 \r\nL 342.33721 275.514036 \r\nL 342.33721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 342.33721 275.516 \r\nL 342.33721 275.516 \r\nL 345.893574 275.516 \r\nL 345.893574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 345.893574 275.516 \r\nL 345.893574 275.513054 \r\nL 349.449937 275.513054 \r\nL 349.449937 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 349.449937 275.516 \r\nL 349.449937 275.513054 \r\nL 353.006301 275.513054 \r\nL 353.006301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 353.006301 275.516 \r\nL 353.006301 275.515018 \r\nL 356.562665 275.515018 \r\nL 356.562665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 356.562665 275.516 \r\nL 356.562665 275.514036 \r\nL 360.119028 275.514036 \r\nL 360.119028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 360.119028 275.516 \r\nL 360.119028 275.516 \r\nL 363.675392 275.516 \r\nL 363.675392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 363.675392 275.516 \r\nL 363.675392 275.515018 \r\nL 367.231756 275.515018 \r\nL 367.231756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 367.231756 275.516 \r\nL 367.231756 275.516 \r\nL 370.788119 275.516 \r\nL 370.788119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 370.788119 275.516 \r\nL 370.788119 275.516 \r\nL 374.344483 275.516 \r\nL 374.344483 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 374.344483 275.516 \r\nL 374.344483 275.516 \r\nL 377.900847 275.516 \r\nL 377.900847 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 377.900847 275.516 \r\nL 377.900847 275.515018 \r\nL 381.45721 275.515018 \r\nL 381.45721 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 381.45721 275.516 \r\nL 381.45721 275.516 \r\nL 385.013574 275.516 \r\nL 385.013574 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 385.013574 275.516 \r\nL 385.013574 275.516 \r\nL 388.569937 275.516 \r\nL 388.569937 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 388.569937 275.516 \r\nL 388.569937 275.516 \r\nL 392.126301 275.516 \r\nL 392.126301 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 392.126301 275.516 \r\nL 392.126301 275.516 \r\nL 395.682665 275.516 \r\nL 395.682665 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 395.682665 275.516 \r\nL 395.682665 275.515018 \r\nL 399.239028 275.515018 \r\nL 399.239028 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 399.239028 275.516 \r\nL 399.239028 275.516 \r\nL 402.795392 275.516 \r\nL 402.795392 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 402.795392 275.516 \r\nL 402.795392 275.516 \r\nL 406.351756 275.516 \r\nL 406.351756 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n    <path clip-path=\"url(#pd2c6188e82)\" d=\"M 406.351756 275.516 \r\nL 406.351756 275.516 \r\nL 409.908119 275.516 \r\nL 409.908119 275.516 \r\nz\r\n\" style=\"fill:#595959;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 241.869938 21.5 \r\nL 417.909937 21.5 \r\nL 417.909937 7.2 \r\nL 241.869938 7.2 \r\nz\r\n\" style=\"fill:#d9d9d9;\"/>\r\n   </g>\r\n   <g id=\"text_20\">\r\n    <!-- effect -->\r\n    <g style=\"fill:#1a1a1a;\" transform=\"translate(317.233062 16.77825)scale(0.088 -0.088)\">\r\n     <defs>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"61.523438\" xlink:href=\"#DejaVuSans-102\"/>\r\n     <use x=\"96.728516\" xlink:href=\"#DejaVuSans-102\"/>\r\n     <use x=\"131.933594\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"193.457031\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"248.4375\" xlink:href=\"#DejaVuSans-116\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pc766597775\">\r\n   <rect height=\"266.112\" width=\"176.04\" x=\"60.789938\" y=\"21.5\"/>\r\n  </clipPath>\r\n  <clipPath id=\"pd2c6188e82\">\r\n   <rect height=\"266.112\" width=\"176.04\" x=\"241.869938\" y=\"21.5\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<ggplot: (-9223371919558747027)>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "token_lengths_cause = []\n",
    "token_lengths_effect= []\n",
    "\n",
    "for dataset in [train, testi, valid]:\n",
    "    token_lengths_cause  = token_lengths_cause  + [len(token_idxs) for token_idxs in dataset['cause_idxs']]\n",
    "    token_lengths_effect = token_lengths_effect + [len(token_idxs) for token_idxs in dataset['effect_idxs']]\n",
    "\n",
    "# where are the 80th percentiles?\n",
    "print('Cause Sentences...')\n",
    "print('The longest sentence has a token length of: ', np.max(token_lengths_cause))\n",
    "cause_nth = np.percentile(token_lengths_cause,  70)\n",
    "print('The 80th percentile of sentences have token length of: ', cause_nth)\n",
    "print('\\n')\n",
    "print('Effect Sentences...')\n",
    "print('The longest sentence has a token length of: ', np.max(token_lengths_effect))\n",
    "effect_nth= np.percentile(token_lengths_effect, 70)\n",
    "print('The 80th percentile of sentences have token length of: ', cause_nth)\n",
    "\n",
    "# chart the distribution of token lengths as a histogram\n",
    "# first we'll need a factor by which to draw the facets of the chart\n",
    "# plottnine does not allow subplots, must use facets for multiple charts\n",
    "token_factor = ['cause' for i in range(len(token_lengths_cause))] + ['effect' for i in range(len(token_lengths_effect))]\n",
    "\n",
    "hist_data = pd.DataFrame({'token_lengths':token_lengths_cause + token_lengths_effect, 'token_factor':token_factor})\n",
    "(\n",
    "    p9.ggplot(hist_data, p9.aes(x='token_lengths'))\n",
    "    + p9.geom_histogram(binwidth=10)\n",
    "    + p9.facet_wrap('token_factor')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "source": [
    "All sentences will have to be padded to the length of the longest sentence in the dataset. This will make for a huge amount of data, of which the vast majority will just be '0', the padding token. It will also increase the number of weights in the model and hence the training time. To simplify the project and work within our resources, we will remove the records with clause length greater than the 60th percentile\n",
    "\n",
    "We will also remove the very short clauses, as they may be single words and other unuseful examples. This min_length has been arbitratily set at 10 tokens. This makes the range fairly narrow, so may assist training. Note, there are always at least 2 tokens, start token and finish token, so 10 tokens actually means 8 tokens of text.\n",
    "\n",
    "The upshot is a clause range of 10 to 20 tokens for both the cause and the effect. This should aid training as all clauses will be of similar length."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before cropping the clauses...\n",
      "train data length: 516382\n",
      "testi data length: 28665\n",
      "valid data length: 28940\n",
      "After cropping clauses to min_length= 10  tokens, and max_length= 23  tokens.\n",
      "train data length: 180092\n",
      "testi data length: 10040\n",
      "valid data length: 10038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_length = int(max(cause_nth, effect_nth))\n",
    "\n",
    "print('Before cropping the clauses...')\n",
    "print('train data length:', len(train))\n",
    "print('testi data length:', len(testi))\n",
    "print('valid data length:', len(valid))\n",
    "\n",
    "# Get mask for rows where both the cause and effects have a number of tokens between a minimum and the maximum.\n",
    "# We don't want sentences too long, neither too short\n",
    "# Minimum length has been arbitarily set to 10 tokens. Max length is calculated above.\n",
    "\n",
    "min_length = 10\n",
    "masks={}\n",
    "for dataset in [train, testi, valid]:\n",
    "    masks[dataset.name] = [ min_length <= len(cause)  <= max_length and\n",
    "                            min_length <= len(effect) <= max_length for \n",
    "                            cause, effect in \n",
    "                            zip(dataset['cause_idxs'], dataset['effect_idxs']) ]\n",
    "\n",
    "# apply those masks to filter out the longer records\n",
    "# also reset the dataframe indexes, to extract batches we'll benefit from sequential indexes starting at 0\n",
    "train = train[masks[train.name]].reset_index()\n",
    "testi = testi[masks[testi.name]].reset_index()\n",
    "valid = valid[masks[valid.name]].reset_index()\n",
    "\n",
    "print('After cropping clauses to min_length=', min_length, ' tokens, and max_length=',max_length,' tokens.')\n",
    "print('train data length:', len(train))\n",
    "print('testi data length:', len(testi))\n",
    "print('valid data length:', len(valid))\n"
   ]
  },
  {
   "source": [
    "## Pad Input Tokens\n",
    "\n",
    "Now we need to apply the padding, so that all token vectors are of length = max_length\n",
    "\n",
    "The index of the padding token for BERT is 0, we need to pad to max number of indexes for the batch\n",
    "\n",
    "The final token must always be 102, so the padding 0s must be inserted before that final token. We could have padded whilst using the tokenizer, but we would end up saving huge files full of zeros.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will submit the data to BERT in batches and already padded \n",
    "\n",
    "def get_batch_for_BERT(batch, max_length=max_length):\n",
    "\n",
    "    # apply the padding to max_length\n",
    "    for i in range(len(batch)):\n",
    "\n",
    "        # get pads to desired max_length\n",
    "        padding = [0] * (max_length - len(batch[i]))\n",
    "\n",
    "        # insert pads to list\n",
    "        batch[i][-1:-1] = padding\n",
    "\n",
    "    # get data in torch tensor format\n",
    "    batch_token_idxs = torch.tensor( batch )\n",
    "\n",
    "    # create the sentence segment ids, always same length, max_length\n",
    "    segment_ids = len(batch) * [[0] * max_length]\n",
    "    batch_segs  = torch.tensor( segment_ids )\n",
    "\n",
    "    return batch_token_idxs, batch_segs"
   ]
  },
  {
   "source": [
    "Let's inspect an example of converting inputs to PyTorch tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of batch tensor for  cause_idxs :  torch.Size([3, 23]) \n\nExample pytorch tensor of tokens for cause_idxs   : \n tensor([  101, 24367,  2245,  1996,  2413,  2052,  2025,  4013,  3630, 17457,\n         1996,  2773,  6892, 11178,     0,     0,     0,     0,     0,     0,\n            0,     0,   102]) \n\nExample pytorch tensor of segments for cause_idxs : \n tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \n\n"
     ]
    }
   ],
   "source": [
    "# specify an example\n",
    "item = 'cause_idxs'\n",
    "data = valid[item]\n",
    "\n",
    "# extracxt an example batch of 3 records\n",
    "# be sure to get a copy of the data, else the code will amend the pandas dataframe in place.\n",
    "batch = data[0:3].tolist().copy()\n",
    "\n",
    "# use the above function to convert the data into a batch for submission to BERT\n",
    "batch_token_idxs, batch_segs = get_batch_for_BERT(batch)\n",
    "\n",
    "print('Size of batch tensor for ',item,': ', batch_token_idxs.size(), '\\n')\n",
    "print('Example pytorch tensor of tokens for',item,'  : \\n', batch_token_idxs[0],'\\n')\n",
    "print('Example pytorch tensor of segments for',item,': \\n', batch_segs[0],'\\n')\n"
   ]
  },
  {
   "source": [
    "## Pad Target Tokens\n",
    "\n",
    "We will seek embeddings only for the inputs. The target will not be embedded, primarily because its difficult to map embeddings back to tokens, hence back to human language. Whereas it is much easier to convert tokens to recognisable English.\n",
    "\n",
    "The target of the model's training will be the token indexes. For example, if we input the embedding of an 'effect' then we would expect the model to output the token indexes for the predicted 'cause'. \n",
    "\n",
    "To enable losses between inputs and predictions we pad the target to the same length as the input, max_length. This is returned as a numpy array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(batch_of_tokens):\n",
    "\n",
    "    # apply the padding to max_length\n",
    "    for i in range(len(batch_of_tokens)):\n",
    "\n",
    "        # get pads to desired max_length\n",
    "        padding = [0] * (max_length - len(batch_of_tokens[i]))\n",
    "\n",
    "        # insert pads to list, just before final token\n",
    "        batch_of_tokens[i][-1:-1] = padding\n",
    "\n",
    "    # convert to numpy\n",
    "    batch_of_tokens = np.array(batch_of_tokens)\n",
    "\n",
    "    return batch_of_tokens"
   ]
  },
  {
   "source": [
    "## Using BERT\n",
    "\n",
    "Calling BertModel.from_pretrained() will fetch the pretrained BERT model from huggingface. When we load the bert-base-uncased, we see the definition of the model printed in the logging. The model is a deep neural network with 12 layers. Explaining the layers and their functions is outside the scope of this post, and you can skip over this output for now.\n",
    "\n",
    "model.eval() puts our model in evaluation mode as opposed to training mode. In this case, evaluation mode turns off dropout regularization which is used in training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "BERT_embedder = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                          output_hidden_states = True # Whether the model returns all hidden-states.\n",
    "                                          )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation (any dropouts will be ignored)\n",
    "BERT_embedder.eval()"
   ]
  },
  {
   "source": [
    "Next, let’s evaluate BERT on our example text, and fetch the hidden states of the network!\n",
    "\n",
    "Side note: torch.no_grad tells PyTorch not to construct the compute graph during this forward pass (since we won’t be running backprop here)–this just reduces memory consumption and speeds things up a little."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = BERT_embedder(batch_token_idxs, batch_segs)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "source": [
    "## Understanding BERT's Output\n",
    "\n",
    "The full set of hidden states for this model, stored in the object hidden_states, is a little dizzying. This object has four dimensions, in the following order:\n",
    "\n",
    "- The layer number (13 layers)\n",
    "- The batch number (1 sentence)\n",
    "- The word / token number (22 tokens in our sentence)\n",
    "- The hidden unit / feature number (768 features)\n",
    "\n",
    "Wait, 13 layers? Doesn’t BERT only have 12? It’s 13 because the first element is the input embeddings, the rest is the outputs of each of BERT’s 12 layers. That’s 219,648 unique values just to represent our one sentence! The second dimension, the batch size, is used when submitting multiple sentences to the model at once; here, though, we just have one example sentence.\n",
    "\n",
    "Grouping the values by layer makes sense for the model, but for our purposes we want it grouped by token.\n",
    "Current dimensions:\n",
    "\n",
    "- \\[# layers, # batch_size, # tokens, # features\\]\n",
    "- e.g. \\[ 0, 1, 2, 3]\n",
    "\n",
    "Desired dimensions:\n",
    "\n",
    "- \\[# batch_size, # tokens, # layers, # features\\]\n",
    "- e.g. \\[ 1, 2, 0, 3]\n",
    "\n",
    "Luckily, PyTorch includes the permute function for easily rearranging the dimensions of a tensor. Let’s combine the layers to make this one whole big tensor. Then we can switch around the “layers” and “tokens” dimensions with permute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor shape: [batch_size= 3 , tokens= 23 , layers= 13 , features= 768 ]\n"
     ]
    }
   ],
   "source": [
    "def batch_stack(hidden_states):\n",
    "    # Concatenate the tensors for all layers. \n",
    "    # We use `stack` here to create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Swap dimensions so we have [batch_size, tokens, layers, features]\n",
    "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "\n",
    "    return token_embeddings\n",
    "\n",
    "batch_embeddings = batch_stack(hidden_states)\n",
    "size = list(batch_embeddings.size())\n",
    "print('Tensor shape: [batch_size=',size[0],', tokens=',size[1],', layers=',size[2],', features=',size[3],']')\n"
   ]
  },
  {
   "source": [
    "## Creating word and sentence vectors from hidden states\n",
    "\n",
    "Now, what do we do with these hidden states? We would like to get individual vectors for each of our tokens, or perhaps a single vector representation of the whole sentence, but for each token of our input we have 13 separate vectors each of length 768.\n",
    "\n",
    "In order to get the individual vectors we will need to combine some of the layer vectors…but which layer or combination of layers provides the best representation?\n",
    "\n",
    "Unfortunately, there’s no single easy answer… Let’s try a couple reasonable approaches, though. Afterwards, I’ll point you to some helpful resources which look into this question further."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Embedding Shape to Feed the Transformer\n",
    "\n",
    "To get embedding vectors we could concatenate the last four layers, giving us a single word vector per token. Each vector would have length 4 x 768 = 3,072. Alternatively, we'll get smaller tensors if we simply sum the vectors, so each vector will retain length 768 (so, quarter the size).\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor shape: [batch_size= 3 , tokens= 23 , features= 768 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_batch_of_sums(batch_embeddings):\n",
    "    \n",
    "    # shape of batch_embeddings  = [batch_size, tokens, layers, features]\n",
    "    # This shape is achieved using a nested list comprehension\n",
    "    # the outer layer of the nested list comprehension cycles thru all records, until 'batch_size' records have been processed\n",
    "    # the shape at this stage is = [tokens, layers, features]\n",
    "    # then the inner layer cycles thru each token and sums the token's values from the final 4 layers (-4:)\n",
    "    # the shape at this stage is = [layers, features]\n",
    "    # the result is a vector [batch_size, tokens, features]\n",
    "    batch_of_sums = [[torch.sum(token[-4:,:], dim=0) for token in record] for record in batch_embeddings]\n",
    "\n",
    "    return batch_of_sums\n",
    "\n",
    "batch_of_sums = get_batch_of_sums(batch_embeddings)\n",
    "print('Tensor shape: [batch_size=',len(batch_of_sums),', tokens=',len(batch_of_sums[0]),', features=',len(batch_of_sums[0][0]),']')\n"
   ]
  },
  {
   "source": [
    "## A Single Function to Get BERT Embeddings\n",
    "\n",
    "Let's put these functions together:\n",
    "\n",
    "- grab a batch of 32 records\n",
    "- get embeddings from BERT\n",
    "- then sum the last 4 layers of the embeddings\n",
    "\n",
    "Then have this data ready to present as the input for a transformer model. The shape to be entered to the model should be \\[batch_size, tokens, features]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of functions\n",
    "\n",
    "def get_embeddings_and_tokens(batch, BERT_embedder, negate_padding=False):\n",
    "\n",
    "    # get tokens (padded to max_length) and segment ids for the batch\n",
    "    batch_token_idxs, batch_segs = get_batch_for_BERT(batch)\n",
    "\n",
    "    # submit those tokens and segment ids to BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = BERT_embedder(batch_token_idxs, batch_segs)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # stack the resulting hidden states from BERT\n",
    "    hidden_states_stacked = batch_stack(hidden_states)\n",
    "\n",
    "    # sum the final 4 layers of the hidden states\n",
    "    embeddings = get_batch_of_sums(hidden_states_stacked)\n",
    "\n",
    "    # embeddings is a list of lists of torch tensors\n",
    "    # convert to numpy for the sake of tensorflow transformer\n",
    "    dim_a = len(batch) # number of examples per batch\n",
    "    dim_b = max_length # number of tokens per example\n",
    "    embeddings_np = np.array( [[ embeddings[i][j].tolist() for j in range(dim_b)] for i in range(dim_a) ] )\n",
    "\n",
    "    # we may want to force our transformer to consider all informaiton in the embeddings\n",
    "    # Whereas by default it will mask out embeddings in the position of padded tokens, ie where the token is 0\n",
    "    # To achieve this, we set all padding (ie 0) tokens to 1 \n",
    "    if negate_padding:\n",
    "        batch_token_idxs = np.where(batch_token_idxs==0, 1, batch_token_idxs)\n",
    "\n",
    "    # return the embeddings and the token indexes, both as numpy\n",
    "    return embeddings_np, batch_token_idxs.numpy()"
   ]
  },
  {
   "source": [
    " Let's run this on an example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to extract 1000 embeddings= 29.872174739837646 seconds\nShape of batch of embeddings of causes  [batch,tokens,features]:  (1000, 23, 768)\n"
     ]
    }
   ],
   "source": [
    "# specify a dataset for the example\n",
    "dataset = valid\n",
    "\n",
    "# extract the batch of causes and the matching batch of effects\n",
    "# be sure to get a copy of the data, else the code will amend the pandas dataframe in place.\n",
    "start_time = time.time()\n",
    "batch_cause  = dataset['cause_idxs'][0:1000].tolist().copy()\n",
    "\n",
    "# get the embeddings by submitting the tokens to the BERT model, returns np.array\n",
    "batch_cause_embed, batch_cause_token = get_embeddings_and_tokens(batch=batch_cause,  BERT_embedder=BERT_embedder, negate_padding=False)\n",
    "\n",
    "# get time\n",
    "print(\"Time to extract 1000 embeddings= %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# print results of example to screen\n",
    "print ('Shape of batch of embeddings of causes  [batch,tokens,features]: ', batch_cause_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=176.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "372f4557f83c4227b3190dd61e7fbbe3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Saved embeddings\n",
      "Embeddings shape =  (180092, 23, 768)\n",
      "---------------------\n",
      "Saved tokens\n",
      "Tokens shape =  (180092, 23)\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# Ouch, 27 seconds for 1000 embeddings. Torch is working in CPU mode, not using the GPU\n",
    "# Each training epoch will need 180,000 embeddings\n",
    "# So, 180k * 27s = 1hr20mins PER EPOCH, just to fetch embeddings\n",
    "# If memory allows then we should precalculate all input embeddings\n",
    "# inputs will be effects, targets will be causes\n",
    "# The current training set of embeddings will be a 25GB file, this server can handle that in memory\n",
    "# If it were much larger then the below code would have to be rewritten using a tool like DASK\n",
    "import math\n",
    "\n",
    "input_item  = 'effect_idxs'\n",
    "batch_size  = 1024\n",
    "batch_qty   = math.ceil(len(train)/batch_size)\n",
    "batch_start = 0\n",
    "\n",
    "if batch_size > len(train):\n",
    "    raise Exception('batch size larger than data')\n",
    "\n",
    "for i in tq.tqdm(range(batch_qty)):\n",
    "\n",
    "    # get batch start/end\n",
    "    batch_end   = batch_start + batch_size\n",
    "    batch_end   = batch_end if batch_end < len(train) else len(train)\n",
    "\n",
    "    # get data from pandas\n",
    "    batch_input = train[input_item][batch_start:batch_end].tolist().copy()\n",
    "\n",
    "    # get the embeddings by submitting the tokens to the BERT model, returns np.arrays\n",
    "    batch_input_embed, batch_input_token = get_embeddings_and_tokens(batch=batch_input,  BERT_embedder=BERT_embedder, negate_padding=False)\n",
    "    \n",
    "    # append the result to numpy array\n",
    "    if i == 0:\n",
    "        train_input_embed = batch_input_embed\n",
    "        train_input_token = batch_input_token\n",
    "    else:\n",
    "        train_input_embed = np.vstack((train_input_embed, batch_input_embed))\n",
    "        train_input_token = np.vstack((train_input_token, batch_input_token))\n",
    "\n",
    "    # increment batch\n",
    "    batch_start = batch_end\n",
    "\n",
    "# save to file\n",
    "np.save(os.path.join(path,'train_input_embed.npy'), train_input_embed, allow_pickle=True)\n",
    "print(\"Saved embeddings\")\n",
    "print(\"Embeddings shape = \", train_input_embed.shape)\n",
    "print(\"---------------------\")\n",
    "np.save(os.path.join(path,'train_input_token.npy'), train_input_token, allow_pickle=True)\n",
    "print(\"Saved tokens\")\n",
    "print(\"Tokens shape = \", train_input_token.shape)\n",
    "print(\"---------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from file. train_input_embed is 25GB with 180,000 training clauses, 23 tokens per clause, 768 embeddings per token !\n",
    "train_input_embed = np.load(os.path.join(path,'train_input_embed.npy'))\n",
    "train_input_token = np.load(os.path.join(path,'train_input_token.npy'))\n"
   ]
  },
  {
   "source": [
    "## Embeddings of Padding Tokens & Start/End Tokens\n",
    "\n",
    "All clauses are now 36 tokens long with each token having an embedding of 768 values. Most of the clauses include some padding, this is where one or more tokens are '0'.\n",
    "\n",
    "BERT returns one embedding for each token in a clause, BERT understands each tpoken in the context of other tokens. So, it returns a different embedding for each starting token (\\[101]), for each ending token (\\[102]) and for each padding token (\\[0]).\n",
    "\n",
    "It is difficult to know whether the BERT embedding for padding tokens is a meaningful 'pregnant silence' within the context of the other tokens. This is possible, or is it simply garbage that can be ignored? The MaskHandler, presented in a subsequent code chunk, is the tool which decides what the model considers and what it does not. That maskhandler builds its masks based on the token indexes for each clause, not on the embeddings. If it sees a 0, for padding, then it produces the mask which ignores data in that position. We can easily feed the maskhandler a dummy vector which has no padding (no 0 tokens), thus forcing the transformer to process all data in the embeddings. This is an option in the get_embeddings_and_tokens() function.\n",
    "\n",
    "Meanwhile, let's inspect the data and see examples of \n",
    "- a) the differing embeddings for starting tokens of separate clauses\n",
    "- b) the differing embeddings for padding tokens within the same clause\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding for the starting token ([101]) in the 1st clause:  [-0.96379304 -0.88908547 -0.70942467 -1.6147567  -1.64514756]\nEmbedding for the starting token ([101]) in the 2nd clause:  [-1.37506938 -2.8377111  -0.42777041 -1.67681956 -2.81532383]\nEmbedding for the starting token ([101]) in the 3rd clause:  [-0.87999892 -1.23370218  0.35404122 -1.2326684  -0.45128268]\n"
     ]
    }
   ],
   "source": [
    "# The first token of all clauses is [101], the starting token\n",
    "# Let's see first 5 values of the different embeddings for three different starting tokens...\n",
    "\n",
    "print('Embedding for the starting token ([101]) in the 1st clause: ', batch_cause_embed[0][0][0:5])\n",
    "print('Embedding for the starting token ([101]) in the 2nd clause: ', batch_cause_embed[1][0][0:5])\n",
    "print('Embedding for the starting token ([101]) in the 3rd clause: ', batch_cause_embed[2][0][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cause text  : \n they are easily available and cheaper \n\nCause tokens: \n[101, 24367, 2245, 1996, 2413, 2052, 2025, 4013, 3630, 17457, 1996, 2773, 6892, 11178, 0, 0, 0, 0, 0, 102] \n\nPenultimate two tokens, should both be padding [0]: \n[0 0] \n\nPenultimate token, a padding embedding, example values 1:5 of 768: \n[-2.73467731 -2.77842164 -1.02185428 -1.43788278  0.38926089] \n\nPrior token, also a padding embedding but with different values, example values 1:5 of 768: \n[ 1.2569313  -2.31451321  2.64633203 -1.93836582  3.66872072] \n\n"
     ]
    }
   ],
   "source": [
    "# here we inspect the differeing embeddings for padding tokens within the same clause\n",
    "# then we confirm our ability to zero out padding embeddings \n",
    "\n",
    "print('Cause text  : ')\n",
    "print(dataset['cause'][0],'\\n')\n",
    "print('Cause tokens: ')\n",
    "print(dataset['cause_idxs'][0],'\\n')\n",
    "print('Penultimate two tokens, should both be padding [0]: ')\n",
    "print(batch_cause_token[0][max_length-3:-1],'\\n')\n",
    "print('Penultimate token, a padding embedding, example values 1:5 of 768: ')\n",
    "print(batch_cause_embed[0][max_length-2][0:5],'\\n')\n",
    "print('Prior token, also a padding embedding but with different values, example values 1:5 of 768: ')\n",
    "print(batch_cause_embed[0][max_length-3][0:5],'\\n')\n"
   ]
  },
  {
   "source": [
    "# Tensorflow Preprocessing\n",
    "\n",
    "The above code chunks extract the training data and inputs are passed through BERT to get embeddings. Those embeddings must presente dto a Transformer model which will train to output the cause of each inputted effect. This Transformer will be built in Tensorflow. Before going into the definition of the model there are a couple of data preprocessing tasks best handled by tensorflow:\n",
    "\n",
    "- The first is positional encoding, to help the model understand the rerlative location of the token\n",
    "\n",
    "- The second is to build that positional encoding into a preprocessing layer.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers               import Layer, Dense, LayerNormalization, Embedding, Dropout\n",
    "from tensorflow.keras.models               import Sequential, Model\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.optimizers           import Adam\n",
    "from tensorflow.keras.losses               import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics              import Mean, SparseCategoricalAccuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## Positional Encoding\n",
    "\n",
    "The meaning of the word (or token, or embedding vector) can depend on the position of that word in a sentence and on its relationship with other words in that same sentence. But the tranformer model contains no recurrence and no convolution, so the sequence in which the data is presented to the model is not usable by the model. For this reason the Transfomer will benefit from additional information about the relative position of every word in a sequence. \n",
    "\n",
    "That information is 'transferred' into the transformer by using a 'positional encoding' vector. This process is proposed in the “Attention is all you need” paper. The positional encoding vector (PE) is simply summed with the embeddings. The functions are sinusoidal, so can be extended over arbitrary clause lengths.\n",
    "\n",
    "- PE<sub>pos,2i</sub> = sin(pos/1000<sup>2i/d</sup>)\n",
    "\n",
    "- PE<sub>pos,2i+1</sub> = cos(pos/1000<sup>2i/d</sup>)\n",
    "\n",
    "- where pos = value for position, i = possible positions, d = embedding_size (dims)\n",
    "\n",
    "For example, consider situation where we have a batch of 3 examples of clauses each with 6 tokens and an embedding size (dimension) of 4 values per token.\n",
    "The positional encoding method will return a tensor with shape (1, 6, 4), this is then broadcast over the batch of (3, 6, 4)\n",
    "\n",
    "Let's see a couple of examples, first with the above simple values and then with the larger real tensor of our embeddings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(object):\n",
    "\n",
    "    def __init__(self, tokens_per_clause, embedding_dim):\n",
    "\n",
    "        angle_rads = self._get_angles(tokens_per_clause  = np.arange(tokens_per_clause)[:, np.newaxis], # get sequence of integers up to tokens_per_clause\n",
    "                                      embedding_sequence = np.arange(embedding_dim)[np.newaxis, :],    # get sequence of integers up to embedding_dims\n",
    "                                      embedding_dim      = embedding_dim)\n",
    "\n",
    "        sines      = np.sin(angle_rads[:, 0::2])\n",
    "        cosines    = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        self._encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "        self._encoding = self._encoding[np.newaxis, ...]\n",
    "    \n",
    "    def _get_angles(self, tokens_per_clause, embedding_sequence, embedding_dim):\n",
    "\n",
    "        angle_rates = 1 / np.power(10000, (2 * (embedding_sequence//2)) / np.float32(embedding_dim))\n",
    "        angle_rads  = tokens_per_clause * angle_rates\n",
    "\n",
    "        return angle_rads\n",
    "    \n",
    "    def get_positional_encoding(self):\n",
    "        \n",
    "        encoding = tf.cast(self._encoding, dtype=tf.float32)\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imagine we have data where the clauses each have 6 tokens. Each token has an embedding vector with 4 values (embedding_dim=4).\nThe positional encoder will return a (1,6,4) tensor with values representing each position in this space.\nThese will later be summed with the input tensor [batch, tokens_per_clause, embedding_dim], so must share same shape:\n———–\ntf.Tensor(\n[[[ 0.          0.          1.          1.        ]\n  [ 0.84147096  0.00999983  0.5403023   0.99995   ]\n  [ 0.9092974   0.01999867 -0.41614684  0.9998    ]\n  [ 0.14112     0.0299955  -0.9899925   0.99955004]\n  [-0.7568025   0.03998933 -0.6536436   0.9992001 ]\n  [-0.9589243   0.04997917  0.2836622   0.99875027]]], shape=(1, 6, 4), dtype=float32)\n———–\n"
     ]
    }
   ],
   "source": [
    "# example usage with simple values:\n",
    "\n",
    "positional_encoding        = PositionalEncoding(tokens_per_clause=6, embedding_dim=4)\n",
    "positional_encoding_values = positional_encoding.get_positional_encoding()\n",
    "\n",
    "print(\"Imagine we have data where the clauses each have 6 tokens. Each token has an embedding vector with 4 values (embedding_dim=4).\")\n",
    "print(\"The positional encoder will return a (1,6,4) tensor with values representing each position in this space.\")\n",
    "print(\"These will later be summed with the input tensor [batch, tokens_per_clause, embedding_dim], so must share same shape:\")\n",
    "print(\"———–\")\n",
    "print(positional_encoding_values)\n",
    "print(\"———–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Positional encoding tensor shape    :  (1, 23, 768)\n",
      "Input embeddings tensor shape       :  (1000, 23, 768)\n",
      "Sum of the two tensors, output shape:  (1000, 23, 768)\n"
     ]
    }
   ],
   "source": [
    "# our real parameters are: tokens_per_clause=36 and embedding_dims=768\n",
    "# Let's see if we can sum the positional_encodings with an exmaple input batch\n",
    "# The positional encodings are of course the same for each record in a batch\n",
    "# so summing involves broadcasting the positional_encodings over the batch\n",
    "\n",
    "positional_encoding        = PositionalEncoding(tokens_per_clause=batch_cause_embed.shape[1], embedding_dim=batch_cause_embed.shape[2])\n",
    "positional_encoding_values = positional_encoding.get_positional_encoding()\n",
    "\n",
    "print('Positional encoding tensor shape    : ', positional_encoding_values.shape)\n",
    "print('Input embeddings tensor shape       : ', batch_cause_embed.shape)\n",
    "print('Sum of the two tensors, output shape: ', (batch_cause_embed + positional_encoding_values).shape)"
   ]
  },
  {
   "source": [
    "## Pre Processing Layer\n",
    "\n",
    "Both the Encoder and the Decoder employ a 'pre processing layer'. To be clear, the individual encoder and decoder blocks do not use this layer, it is applied only once for encoding and once for decoding.\n",
    "\n",
    "The layer incorporates a standard keras embedding layer. An embedding layer is just a trainable look-up table: we give it a token (integer), which is the index of the word in the vocabulary, and it returns a trainable word-vector (embedding) for that index. The dimension of the vector returned by the embedding layer is set by embedding_dim, the same value is used in the encoder and decoder layers but referred to as num_neurons. In 'Attention is all you need' this dimension is 512, whereas we use 768 due this being the embedding size from BERT. This dimension is the same throughout the layers of the transformer. Note, a transformer is NOT an autoencoder where the encoder progressively shrinks the dims and the decoder progressively grows them.\n",
    "\n",
    "To this embedding the layer then adds the above 'positional encoding'.\n",
    "\n",
    "At the end of layer there is dropout, in order to avoid over-fitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessingLayer(Layer):\n",
    "\n",
    "    def __init__(self, tokens_per_clause, vocabulary_size, embedding_dim):\n",
    "\n",
    "        super(PreProcessingLayer, self).__init__()\n",
    "\n",
    "        # Initialize\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # instantiate embedding layer\n",
    "        self.embedding = Embedding(vocabulary_size, self.embedding_dim)\n",
    "        \n",
    "        # get positional encoding, calculated up front and used on all tensors passing thru\n",
    "        positional_encoding_handler = PositionalEncoding(tokens_per_clause=tokens_per_clause, embedding_dim=self.embedding_dim)\n",
    "        self.positional_encoding    = positional_encoding.get_positional_encoding()\n",
    "\n",
    "        # instantiate dropout\n",
    "        self.dropout = Dropout(0.1)\n",
    "    \n",
    "    def call(self, inputs, training, apply_embedding):\n",
    "\n",
    "        # IF inputting BERT embeddings with shape [batch, tokens_per_clause, BERT_embedding_dim]\n",
    "        # then SKIP the embedding layer because the data is already embedded!\n",
    "        # IF inputting integer indices, one per word token, then the embedding layer is required\n",
    "        # Also, this embedding layer MUST remain for a layer instance serving a Decoder, even if using BERT for input\n",
    "        # The layer learns an embedding for the input sequence, the output shape will be [batch, tokens_per_clause, embedding_dim]\n",
    "        if apply_embedding:\n",
    "            inputs = self.embedding(inputs)\n",
    "        \n",
    "        # this line is as per a recommendation in the Attention is all you need paper\n",
    "        sequence = inputs * tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        # add embedding to positional encoding\n",
    "        sequence = sequence + self.positional_encoding\n",
    "\n",
    "        # apply dropout, if in training mode\n",
    "        sequence = self.dropout(sequence, training=training)\n",
    "        \n",
    "        return sequence"
   ]
  },
  {
   "source": [
    "# Tensorflow Transformer\n",
    "\n",
    "The above code chuinks prepare the data. We may now move to defining the model. The transformer will be fairly standard and trained from scratch. It is possible to fine tune an existing transformer for many tasks, such as classification, as is described in the huggingface documentation at https://huggingface.co/transformers/master/custom_datasets.html. However, for text prediction tasks it may be better to train from scratch\n",
    "\n",
    "The input will be the effect as an embedding. The target will be the cause as tokens. The predicted tokens will be easy to translate back to words, whereas BERT embeddings are not so easy to translate back to words as they are context specific.\n",
    "\n",
    "Note, it could be trained the other way around, the cause could be the input and the target would be the effect"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TF2 Transformer From Scratch\n",
    "\n",
    "Follows the standard TensorFlow Transformer for translation example:\n",
    "https://github.com/tensorflow/tensor2tensor\n",
    "\n",
    "Which was then adapted by N Zivkovic and N Djavic for a translation model, see link:\n",
    "https://rubikscode.net/2019/09/30/transformer-series/\n",
    "\n",
    "The code by Zivkovic and Djavic has then been adapted for use in this model. Their code is for a translation tool which learns its own embeddings, whereas this model is a sentence prediction tool which has embeddings submitted from BERT. Therefore, many changes have been made to adapt the code accordingly.\n",
    "\n",
    "Whereas the BERT model was from PyTorch, this Transformer will be back in my homeground of TensorFlow, as it will be coded from scratch not simply fine tuning of an existing model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Masks\n",
    "\n",
    "The heart of the Trensformer is the ScaledDotProductAttentionLayer class, which is used multiple times throughout the model. We need a mechanism to tell to this layer to ignore any padding in the data, because padding doesn’t carry any information. Our padding was originally represented by a '0' token, but BERT returns embedding values for 0 tokens. So, in the 'train' function (the last piece of code herein) we will use the tokens to derive a mask to pass to the Transformer, so it can know which embeddings have zero information. \n",
    "\n",
    "This assumes the BERT embeddings for padding tokens really do carry no information. We could easily ask the model the ignore this problem and assume all data is valid, this would be done by submitting a dummy vector to the maskhandler which has no padding\n",
    "\n",
    "A second problem is that we must indicate to the model that it should only process words before the current word. This means that to predict the fourth word, only the first, second and the third word are used, but not the fifth.\n",
    "\n",
    "Both of these problems, padding and future words, are solved using masks. For this purpose, we create the class MaskHandler:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskHandler(object):\n",
    "\n",
    "    def padding_mask(self, inputs):\n",
    "        # The batch of sequences used to produce the mask will always be token sequences, not embeddings.\n",
    "        # These batches of tokens have the shape: [batch, tokens_per_clause]\n",
    "        # The masks produced herein are consumed by the ScaledDotProductAttentionLayer\n",
    "        # This layer operates long after the data is embedded, either via BERT or via a keras embedding layer\n",
    "        # so the mask is being applied to tokens, which have shape [batch, tokens_per_clause]\n",
    "\n",
    "        # create a mask for padding tokens, note inverted. 1 for mask, 0 for pass\n",
    "        sequence = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
    "\n",
    "        # expand dims\n",
    "        sequence = sequence[:, tf.newaxis, tf.newaxis, :]\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "    def look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding shape: (2, 4)\nPadding Mask Example:\ntf.Tensor(\n[[[[0. 0. 1. 1.]]]\n\n\n [[[0. 0. 0. 0.]]]], shape=(2, 1, 1, 4), dtype=float32)\n———–\nLook Ahead Mask Example:\ntf.Tensor(\n[[0. 1. 1. 1.]\n [0. 0. 1. 1.]\n [0. 0. 0. 1.]\n [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n———–\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "maskHandler = MaskHandler()\n",
    "\n",
    "# a record is a series of embedding values for each token in a series of tokens\n",
    "# so the shape for a batch of clauses will be [batch, tokens_per_clause] \n",
    "# Let's assume a simple example where batch=2, tokens_per_clause=4\n",
    "# note the first example clause includes two padding tokens; 0, 0 \n",
    "input_tokens = tf.constant([ [7, 7, 0, 0], \n",
    "                             [1, 3, 4, 1] ])\n",
    "\n",
    "print('Embedding shape:', input_tokens.shape)\n",
    "\n",
    "mask = maskHandler.padding_mask(inputs=input_tokens)\n",
    "print('Padding Mask Example:')\n",
    "print(mask)\n",
    "print('———–')\n",
    "\n",
    "mask = maskHandler.look_ahead_mask(size=input_tokens.shape[1])\n",
    "print('Look Ahead Mask Example:')\n",
    "print(mask)\n",
    "print('———–')\n"
   ]
  },
  {
   "source": [
    "## Attention Layers\n",
    "\n",
    "Attention is a concept that allows a Transformer to focus on specific parts of the sequence, i.e. sentence. It can be described as a mapping function, it maps a query and a set of key-value pairs to an output. Query, keys, values, and output are all vectors. The output is calculated as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. The whole process can be divided into 6 steps:\n",
    "\n",
    "- For each item of the input sequence three vectors are calculated: Query – q, Key – k and Value – v. They are calculated by applying three randomly initiated matrices, which are then trained. So Q, K and V are single Dense layers, so called 'linear' layers.\n",
    "\n",
    "- Calculate the score for each item in the input sequence as the dot product of the Query vector versus each Key vector of the other items in the sequence. NB dot product outputs a scalar. \n",
    "    - For example, if we are calculating self-attention for the word “You”, the process creates a score for 'You' versus each word in the sentence. Meaning we calculate dot products of q(You) and k(You), q(You) and k(are) and q(You) and k(awesome). Note, the process scores the word against itself, hence 'self attention'.\n",
    "\n",
    "- Divide the scores by 8 (other values could be used, but this is the default)\n",
    "\n",
    "- Apply the softmax function. That way scores are standardised, positive and add up to 1\n",
    "\n",
    "- Multiply the Value vector with the softmaxed score\n",
    "\n",
    "- Sum all the results into a single vector thus we create the output of 'self-attention'.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The attention used in a Transformer is best known as Scaled Dot-Product Attention. As in other attention layers, the input of this layer contains queries, keys (dimension d<sub>k</sub>) and values (dimension d<sub>v</sub>). We calculate the dot products of the query with all keys. Then we divide each by square root of dk and apply softmax. Mathematically this layer can be described with the formula:\n",
    "\n",
    "Attention(Q,K,V) = softmax<sub>k</sub> (QK<sup>T</sup>/sqrt(d<sub>k</sub>))V\n",
    "\n",
    "where<br>\n",
    "d<sub>k</sub> is the dimension of the input of queries and keys<br>\n",
    "d<sub>v</sub> is the dimension of the input of values<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttentionLayer():\n",
    "\n",
    "    def calculate_output_weights(self, q, k, v, mask):\n",
    "\n",
    "        qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "\n",
    "        scaled_attention = qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention += (mask * -1e9)  \n",
    "\n",
    "        weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
    "        output  = tf.matmul(weights, v)\n",
    "\n",
    "        return output, weights"
   ]
  },
  {
   "source": [
    "## Multi-Head Attention Layer\n",
    "\n",
    "Building on top of the previous layer, it has been noticed that when we linearly project the queries, keys and values n-times with different weight matrices and then concatenate the results, we get better performance. \n",
    "\n",
    "This means that instead of one single attention “head” (i.e. Scaled Dot-Product Layer), Q, K, and V are split into multiple “heads”. This way the model can attend to information at different positions from different 'representational' spaces, where these spaces attend to different language tasks given the model objective.\n",
    "\n",
    "First this layer creates Q, K and V as dense layers in their own right, with their own weights. The size of each dense layer is the same, and is set by the param 'num_neurons'. The MHA custom layer passes the inputs through these dense layers and then splits them into three branches or “heads”. Each head gets the same values of Q, K and V. Then scaled dot-product attention is applied on each “head”, using its own weight matrix W. Results are then concatenated and passed to the final linear layer. Mathematically Multi-Head Attention Layer can be presented with the formula:\n",
    "\n",
    "Multihead(Q, K, V) = Concat(head_1, ...heand_h)W<sup>O</sup>\n",
    "<br> where <br>\n",
    "head_i = Attention(QW<sup>Q</sup><sub>i</sub>, KW<sup>K</sup><sub>i</sub>, VW<sup>V</sup><sub>i</sub>)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, num_neurons, num_heads):\n",
    "        \n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        \n",
    "        self.num_heads       = num_heads\n",
    "        self.num_neurons     = num_neurons \n",
    "        self.depth           = num_neurons // self.num_heads\n",
    "        self.attention_layer = ScaledDotProductAttentionLayer()\n",
    "\n",
    "        if num_neurons % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {num_neurons} should be divisible by number of heads = {num_heads}\")\n",
    "            # so num_heads must be a factor of embedding_dim.\n",
    "            # If embedding_dim=768 then the factors are: 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256, 384\n",
    "        \n",
    "        # Here's qeury q, key k and value v\n",
    "        # all are dense layers with linear activation, which is the keras default\n",
    "        self.q_layer = Dense(num_neurons)\n",
    "        self.k_layer = Dense(num_neurons)\n",
    "        self.v_layer = Dense(num_neurons)\n",
    "\n",
    "        self.linear_layer = Dense(num_neurons)\n",
    "\n",
    "    def split(self, x, batch_size):\n",
    "\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, v, k, q, mask):\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # Run through linear layers\n",
    "        q = self.q_layer(q)\n",
    "        k = self.k_layer(k)\n",
    "        v = self.v_layer(v)\n",
    "\n",
    "        # Split the heads\n",
    "        q = self.split(q, batch_size)\n",
    "        k = self.split(k, batch_size)\n",
    "        v = self.split(v, batch_size)\n",
    "\n",
    "        # Run through attention\n",
    "        # The attention layer applies softmax, which is the first non-linear activation of this data\n",
    "        attention_output, weights = self.attention_layer.calculate_output_weights(q, k, v, mask)\n",
    "        \n",
    "        # Prepare for the rest of processing\n",
    "        output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.num_neurons))\n",
    "        \n",
    "        # Run through final linear layer\n",
    "        output = self.linear_layer(concat_attention)\n",
    "\n",
    "        return output, weights"
   ]
  },
  {
   "source": [
    "## Encoder, Decoder\n",
    "\n",
    "There are typically 6 decoder blocks stacked on each other followed by 6 decoder blocks. This is not an autoencoder, each encoder block does not have a smaller output than the last. In a transformer all sub-layers in the model, as well as the embedding layers, produce outputs of the same dimension. In the original paper this was d<sub>model</sub> = 512, in our model it will be 768 to match BERT's embedding dim.\n",
    "\n",
    "Each ENcoder block has the following layers: \n",
    "    \n",
    "- MultiheadSelfAttention -> Dropout -> Add+Normalise -> \n",
    "- Dense -> Dense -> Dropout -> Add+Normalise\n",
    "\n",
    "Each DEcoder block has the following layers: \n",
    "    \n",
    "- MultiheadSelfAttention -> Dropout -> Add+Normalise -> \n",
    "- MultiheadSelfAttention -> Dropout -> Add+Normalise -> \n",
    "- Dense -> Dense -> Dropout -> Add+Normalise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Encoder Block\n",
    "\n",
    "Encoder = \n",
    "MultiheadSelfAttention -> Add+Normalise -> 2DenseLayers -> Dropout -> Add+Normalise\n",
    "\n",
    "\n",
    "The two dense layers are called the 'feed forward network'.\n",
    "\n",
    "In order to reduce training time, instead of using batch normalization, as we would for standard feed forward neural networks, we use modified approach called **layer normalization**. \n",
    "\n",
    "For reference, batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.\n",
    "\n",
    "In layer normalisation we compute the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in **recurrent networks**.\n",
    "\n",
    "This is all hidden from us if we use LayerNormalization from tensorflow.keras.layers.\n",
    "\n",
    "Effectively, this means that we use layer normalization after each Multi-Head Attention or Feed Forward Neural Network Layer. Also, we use Dropout layer to avoid over-fitting. So, we create two helper functions that will build these layer combinations for us:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_head_attention_layers(num_neurons, num_heads):\n",
    "\n",
    "    # Multihead -> dropout -> layer normalisation\n",
    "    multi_head_attention_layer = MultiHeadAttentionLayer(num_neurons, num_heads)   \n",
    "    dropout                    = tf.keras.layers.Dropout(0.1)\n",
    "    normalization              = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    return multi_head_attention_layer, dropout, normalization\n",
    "\n",
    "def build_feed_forward_layers(num_neurons, num_hidden_neurons):\n",
    "\n",
    "    feed_forward_layer = tf.keras.Sequential()\n",
    "\n",
    "    # Two dense layers -> dropout -> layer normalisation\n",
    "    feed_forward_layer.add(Dense(num_hidden_neurons, activation='relu'))\n",
    "    feed_forward_layer.add(Dense(num_neurons))\n",
    "    dropout       = Dropout(0.1)\n",
    "    normalization = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    return feed_forward_layer, dropout, normalization"
   ]
  },
  {
   "source": [
    "Now we can build encoder blocks, each comprised of sub layers which are discussed above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, num_neurons, num_hidden_neurons, num_heads):\n",
    "\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # Instantiate multi head attention layer, with associated dropout and normalization\n",
    "        self.multi_head_attention_layer, self.attention_dropout, self.attention_normalization = \\\n",
    "        build_multi_head_attention_layers(num_neurons, num_heads)   \n",
    "            \n",
    "        # Instantiate feed-forward network, with associated dropout and normalization\n",
    "        self.feed_forward_layer, self.feed_forward_dropout, self.feed_forward_normalization = \\\n",
    "        build_feed_forward_layers(num_neurons, num_hidden_neurons)\n",
    "       \n",
    "    def call(self, inputs, training, mask):\n",
    "\n",
    "        # Calculate attention output, encode using the padding mask\n",
    "        attention_output, _ = self.multi_head_attention_layer(inputs=None, v=inputs, k=inputs, q=inputs, mask=mask)\n",
    "        # apply dropout\n",
    "        attention_output    = self.attention_dropout(attention_output, training=training)\n",
    "        # apply normalization to sum of this MHA block AND inputs\n",
    "        attention_output    = self.attention_normalization(inputs + attention_output)\n",
    "        \n",
    "        # Calculate output of feed forward network\n",
    "        output = self.feed_forward_layer(attention_output)\n",
    "        # apply dropout\n",
    "        output = self.feed_forward_dropout(output, training=training)\n",
    "        # apply normalization to sum of this feed forward block AND above MHA block\n",
    "        output = self.feed_forward_normalization(attention_output + output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "source": [
    "## Decoder Block\n",
    "\n",
    "The decoder layer is somewhat more complicated, because it has an additional Multi-Head Attention block\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(Layer):\n",
    "\n",
    "    def __init__(self, num_neurons, num_hidden_neurons, num_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # Instantiate multi head attention (MHA), with associated dropout and normalization\n",
    "        self.multi_head_attention_layer1, self.attention_dropout1, self.attention_normalization1 =\\\n",
    "        build_multi_head_attention_layers(num_neurons, num_heads)   \n",
    "        \n",
    "        # The decoder needs a second multihead attention block, with associated dropout and normalization\n",
    "        self.multi_head_attention_layer2, self.attention_dropout2, self.attention_normalization2 =\\\n",
    "        build_multi_head_attention_layers(num_neurons, num_heads)           \n",
    "\n",
    "        # Instantiate feed-forward network, with associated dropout and normalization\n",
    "        self.feed_forward_layer, self.feed_forward_dropout, self.feed_forward_normalization =\\\n",
    "        build_feed_forward_layers(num_neurons, num_hidden_neurons)\n",
    "\n",
    "    def call(self, inputs, encoder_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        # Decoder first MHA block\n",
    "        attention_output1, attention_weights1 = self.multi_head_attention_layer1(inputs=None, v=inputs, k=inputs, q=inputs, mask=look_ahead_mask)\n",
    "        ## apply dropout \n",
    "        attention_output1 = self.attention_dropout1(attention_output1, training=training)\n",
    "        ## apply normalization to sum of this MHA block AND inputs\n",
    "        attention_output1 = self.attention_normalization1(inputs + attention_output1)\n",
    "\n",
    "        # Decoder second MHA block\n",
    "        attention_output2, attention_weights2 = self.multi_head_attention_layer2(inputs=None, v=encoder_output, k=encoder_output, q=attention_output1, mask=padding_mask)\n",
    "        ## apply dropout\n",
    "        attention_output2 = self.attention_dropout1(attention_output2, training=training)\n",
    "        ## apply normalization to sum of this MHA block AND above MHA block\n",
    "        attention_output2 = self.attention_normalization1(attention_output1 + attention_output2)\n",
    "\n",
    "        # Decoder feed forward network\n",
    "        output = self.feed_forward_layer(attention_output2)\n",
    "        ## apply dropout\n",
    "        output = self.feed_forward_dropout(output, training=training)\n",
    "        ## apply normalization to sum of this feed forward block AND above MHA block\n",
    "        output = self.feed_forward_normalization(attention_output2 + output)\n",
    "\n",
    "        return output, attention_weights1, attention_weights2"
   ]
  },
  {
   "source": [
    "## Encoder + Decoder\n",
    "\n",
    "In the “Attention is all you need” paper, authors suggest that we should use 6 Encoder blocks for building the Encoder and 6 Decoder blocks for building the Decoder. This is of course arbitrary, so we use a parameter to define how many encoder blocks there should be. \n",
    "\n",
    "Here is how the Encoder class, which encompasses the encoder blocks, now looks:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "\n",
    "    def __init__(self, num_neurons, num_hidden_neurons, num_heads, tokens_per_clause, vocabulary_size, num_enc_blocks = 6):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # How many encoder blocks to use...\n",
    "        self.num_enc_blocks = num_enc_blocks\n",
    "\n",
    "        # pre processing layer(self, tokens_per_clause, vocabulary_size, embedding_dim)\n",
    "        self.pre_processing_layer = PreProcessingLayer(tokens_per_clause= tokens_per_clause, \n",
    "                                                       vocabulary_size  = vocabulary_size,\n",
    "                                                       embedding_dim    = num_neurons)\n",
    "\n",
    "        # There will be multiple encoder blocks, held in a list\n",
    "        # Each of the feed forward blocks are of the SAME size\n",
    "        # not shrinking, nor growing, as they might in a autoencoder.\n",
    "        self.encoder_blocks = [EncoderBlock(num_neurons, num_hidden_neurons, num_heads) for _ in range(num_enc_blocks)]\n",
    "\n",
    "    def call(self, inputs, training, mask, apply_embedding):\n",
    "\n",
    "        # pre processing (embedding and positional vectors)\n",
    "        sequence = self.pre_processing_layer(inputs         = inputs, \n",
    "                                             training       = training, \n",
    "                                             apply_embedding= apply_embedding)\n",
    "\n",
    "        # The encoder is then comprised of a number of encoder blocks...\n",
    "        # for each encoder block\n",
    "        for i in range(self.num_enc_blocks):\n",
    "            # execute the block on the output of the previous block\n",
    "            sequence = self.encoder_blocks[i](inputs  = sequence, \n",
    "                                              training= training, \n",
    "                                              mask    = mask)\n",
    "\n",
    "        return sequence"
   ]
  },
  {
   "source": [
    "...and for the decoder, which encompasses a number of decoder blocks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "\n",
    "    def __init__(self, num_neurons, num_hidden_neurons, num_heads, vocabulary_size, tokens_per_clause, num_dec_blocks=6):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # How many decoder blocks to use...\n",
    "        self.num_dec_blocks = num_dec_blocks\n",
    "        \n",
    "        # Decoder also has preprocessing, because the decoded sentence embeddings prior to the current word are fed back into the decoder\n",
    "        self.pre_processing_layer = PreProcessingLayer(tokens_per_clause= tokens_per_clause, \n",
    "                                                       vocabulary_size  = vocabulary_size,\n",
    "                                                       embedding_dim    = num_neurons)\n",
    "\n",
    "        # There will be multiple decoder blocks, held in a list\n",
    "        # Each of the feed forward blocks are of the SAME size\n",
    "        # not shrinking, nor growing, as they might in an autoencoder.\n",
    "        self.decoder_blocks = [DecoderBlock(num_neurons, num_hidden_neurons, num_heads) for _ in range(num_dec_blocks)]\n",
    "\n",
    "    def call(self, inputs, encoder_output, training, look_ahead_mask, padding_mask, apply_embedding):\n",
    "\n",
    "        # pre processing. The decoder must always use the embedding layer, so this is defaults to True\n",
    "        sequence = self.pre_processing_layer(inputs         = inputs,\n",
    "                                             training       = training,\n",
    "                                             apply_embedding= apply_embedding)\n",
    "        \n",
    "        # instantiate an empty dictionary to collect the decoder's attention weights\n",
    "        attention_weights = {}\n",
    "\n",
    "        # The decoder is then comprised of a number of decoder blocks...\n",
    "        # ...so, for each decoder block...\n",
    "        for i in range(self.num_dec_blocks):\n",
    "\n",
    "            # execute the block on the output of the previous block                  \n",
    "            sequence, attention_weights1, attention_weights2 = self.decoder_blocks[i](inputs         = sequence, \n",
    "                                                                                      encoder_output = encoder_output, \n",
    "                                                                                      training       = training, \n",
    "                                                                                      look_ahead_mask= look_ahead_mask, \n",
    "                                                                                      padding_mask   = padding_mask)\n",
    "\n",
    "            # Different to the encoder, we will need the decoder's attention weights for the training process ...\n",
    "\n",
    "            # ... get the weights from the decoder block's first MHA\n",
    "            attention_weights['decoder_layer{}_attention_weights1'.format(i+1)] = attention_weights1\n",
    "\n",
    "            # ... get the weights from the decoder block's second MHA       \n",
    "            attention_weights['decoder_layer{}_attention_weights2'.format(i+1)] = attention_weights2\n",
    "\n",
    "        \n",
    "        return sequence, attention_weights"
   ]
  },
  {
   "source": [
    "## The Transformer = Encoder + Decoder\n",
    "\n",
    "We combine the encoder and decoder into a Transformer class and add the final Linear layer (ie a single dense layer) on top of that. \n",
    "\n",
    "It is important to notice that we inherited the Model class for the 'Transformer', because we are able to perform training and get predictions using this class. Apart from that, note that we need to pass on the masks that Encoder and Decoder use during the training process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "\n",
    "    def __init__(self, num_neurons, num_hidden_neurons, num_heads, tokens_per_clause, input_vocabulary_size, target_vocabulary_size, num_blocks):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # instantiate encoder\n",
    "        self.encoder = Encoder(num_neurons       = num_neurons, # same as embedding_dim\n",
    "                               num_hidden_neurons= num_hidden_neurons, \n",
    "                               num_heads         = num_heads, \n",
    "                               tokens_per_clause = tokens_per_clause,\n",
    "                               vocabulary_size   = input_vocabulary_size, \n",
    "                               num_enc_blocks    = num_blocks)\n",
    "\n",
    "        # instantiate decoder\n",
    "        self.decoder = Decoder(num_neurons       = num_neurons, # same as embedding_dim\n",
    "                               num_hidden_neurons= num_hidden_neurons, \n",
    "                               num_heads         = num_heads, \n",
    "                               tokens_per_clause = tokens_per_clause,\n",
    "                               vocabulary_size   = target_vocabulary_size, \n",
    "                               num_dec_blocks    = num_blocks)\n",
    "\n",
    "        # instantiate final dense layer to give probability distributions for each token\n",
    "        # There will be 30522 possible tokens for BERT English, this is the vocabulary size\n",
    "        # so the shape of predictions will be [batch, tokens_per_clause, vocabulary_size]. We will get a probability distribution per token.\n",
    "        # Note, this is different to the targets, whose shape will be [batch, tokens_per_clause]. No probability ditribution, the value is certain.\n",
    "        # This doesn't matter because our loss function wil be SparesCategoricalCrossEntropy with_logits=False,\n",
    "        # This loss function assumes we provide a prob distribution for predictions and precise values for target. It then returns a loss per token\n",
    "        self.linear_layer = Dense(target_vocabulary_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training, encoder_padding_mask, look_ahead_mask, decoder_padding_mask, apply_embedding_enc, apply_embedding_dec):\n",
    "\n",
    "        encoder_output = self.encoder(inputs   = inputs, \n",
    "                                      training = training, \n",
    "                                      mask     = encoder_padding_mask,\n",
    "                                      apply_embedding = apply_embedding_enc)\n",
    "\n",
    "        # decode (using the multiple decoder blocks that form a decoder model)\n",
    "        # also get the attention weights!\n",
    "        decoder_output, attention_weights = self.decoder(inputs          = inputs,\n",
    "                                                         encoder_output  = encoder_output, \n",
    "                                                         training        = training, \n",
    "                                                         look_ahead_mask = look_ahead_mask, \n",
    "                                                         padding_mask    = decoder_padding_mask,\n",
    "                                                         apply_embedding = apply_embedding_dec)\n",
    "\n",
    "        # final dense layer\n",
    "        output = self.linear_layer(decoder_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "source": [
    "## Learning Rate Variation\n",
    "\n",
    "Since we are following the “Attention is all you need” paper we use the Adam optimizer, as the authors of the article suggested. However, that paper also recommends that the learning rate varies, so we create a custom scheduler to do this.\n",
    "\n",
    "### Scheduler and Optimizer\n",
    "\n",
    "The formula used for changing the learning rate during training is:\n",
    "\n",
    "lrate = d<sup>-0.5</sup><sub>model</sub> . min(step_num<sup>-0.5</sup>,  step_num . warmup_steps<sup>-1.5</sup> )\n",
    "\n",
    "In a nutshell, the learning rate is increasing in the first part of the training. Namely, it is increasing until the number of training steps reaches the number 'warmup_steps'. After that learning rate decreases. This decrease is proportional to the 1/sqrt(step_num). \n",
    "\n",
    "In this paper, the value 4000 is used for warmup_steps, so we are doing the same. This means that for the first 4000 steps the learning rate will increase and then it will slowly downgrade.\n",
    "\n",
    "This is implemented in the Schedule class:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schedule(LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, num_neurons, warmup_steps=4000):\n",
    "\n",
    "        super(Schedule, self).__init__()\n",
    "\n",
    "        self.num_neurons = tf.cast(num_neurons, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        lrate = tf.math.rsqrt(self.num_neurons) * tf.math.minimum(arg1, arg2)\n",
    "        return lrate"
   ]
  },
  {
   "source": [
    "Note that the above class inherits from LearningRateSchedule. Because of this we can pass an object of this class into the optimizer object and control the learning rate during the training process, like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_neurons is d_model, in 'Attention Is All You Need' it was set to 512\n",
    "# It is also the embedding dimension, which is 768 for BERT base\n",
    "# If rolling your own embeddings, then you can set this figure to something smaller\n",
    "num_neurons = 8\n",
    "\n",
    "learning_rate = Schedule(num_neurons=num_neurons)\n",
    "optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "source": [
    "## Padding the Loss Function\n",
    "\n",
    "Since all sequences are padded, we need to apply a padding mask when the loss is calculated, so the prediction and target are padded to the same length.\n",
    "\n",
    "As an objective function SparseCategoricalCrossentropy is used and it is padded using the 'padded_lossfunction' function.\n",
    "\n",
    "Why SparseCategoricalCrossentropy?\n",
    "\n",
    "Our tokens are 'categorical', one per word. Tokens are integers, there is not a continuos space of tokens. We will need a categorical loss function. There are so many tokens (categories) that its not reasonable to convert them to one-hot representations. So, we cannot use 'categorical cross entropy' to calculate the loss. \n",
    "\n",
    "Therefore, we use Sparse Categorical Crossentropy. See how it is calculated at: https://leakyrelu.com/2020/01/01/difference-between-categorical-and-sparse-categorical-cross-entropy-loss-function/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_objective_function = SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "# from_logits; logits are outputs which have NOT been normalized via Softmax or Sigmoid.\n",
    "# In our case, we are providing a probability distribution because we have used softmax, so from_logits=False\n",
    "# reduction;  normally set to 'auto', which computes the categorical cross-entropy as the average of label*log(pred). \n",
    "# Setting the value to 'none' gives us a loss for each token. Computing a reduce_mean on this list would give us the same result as with reduction='auto'.\n",
    "# We leave as 'none' because we want to apply a mask, because we don't want to compute losses on targetted padding. \n",
    "# We added the padding to the target, errors (loss) versus this padding should be ignored in training\n",
    "\n",
    "def padded_loss_function(targets, predictions):\n",
    "    \n",
    "    # get a mask of where the prediction matches the target\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "\n",
    "    # apply SparseCategoricalCrossentropy\n",
    "    # to the target vs the prediction\n",
    "    loss = loss_objective_function(targets, predictions)\n",
    "\n",
    "    # apply the mask\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = loss * mask\n",
    "\n",
    "    # 'reduce' the loss from one loss per token to a single value per clause\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "training_loss = Mean(name='training_loss')\n",
    "training_accuracy = SparseCategoricalAccuracy(name='training_accuracy')"
   ]
  },
  {
   "source": [
    "## Training Process\n",
    "\n",
    "Finally we can start the training process. First we need to initialize all necessary parameters and instantiate the an object of the Transformer class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate helpers\n",
    "maskHandler = MaskHandler()\n",
    "\n",
    "## INITIALISE PARAMETERS\n",
    "# num_blocks is the number of encoder blocks in the Encoder\n",
    "# It is also the number of decoder blocks in the Decoder\n",
    "num_blocks = 4\n",
    "\n",
    "# num_neurons is d_model, in 'Attention Is All You Need' it was set to 512\n",
    "# It is also the embedding dimension, which is 768 for BERT base\n",
    "# If rolling your own embeddings, then you can set this figure to something smaller\n",
    "num_neurons = 768\n",
    "\n",
    "# num_hidden_neurons is the hidden dimension in the two dense layers which form the feed forward layers in the MHA, \n",
    "# It is typically larger than num_neurons\n",
    "num_hidden_neurons = 1024\n",
    "\n",
    "# num_heads is the number of attention heads, into which q,k and v are split.\n",
    "# In the original paper this is set to 8, more is not necessarily better\n",
    "# There is a constraint, num_heads must be a factor of d_model; num_neurons % num_heads = 0\n",
    "# In our case, 768 / 8 = 96 \n",
    "num_heads = 8\n",
    "\n",
    "# vocabulary_size, the number of possible tokens given by the tokenizer\n",
    "# If not using BERT embeddings, but rather rolling our own embeddings by inputting the tokens, \n",
    "# then this value is used by our own embedding layer, as defined in the pre-processing function\n",
    "# In translation models we would use a tokenizer for each language so would have two vocabulary sizes; input and target\n",
    "# The 'BERT Base uncased' tokenizer we used above is based on the following number of tokens for English\n",
    "vocabulary_size = 30522\n",
    "\n",
    "# Instantiate learning rate objects\n",
    "learning_rate = Schedule(num_neurons)\n",
    "optimizer     = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Instantiate transformer\n",
    "transformer = Transformer(  num_blocks            = num_blocks, \n",
    "                            num_neurons           = num_neurons, \n",
    "                            num_hidden_neurons    = num_hidden_neurons,\n",
    "                            num_heads             = num_heads,\n",
    "                            tokens_per_clause     = max_length,\n",
    "                            input_vocabulary_size = vocabulary_size, \n",
    "                            target_vocabulary_size= vocabulary_size)\n"
   ]
  },
  {
   "source": [
    "## Train_step function.\n",
    "\n",
    "This is the TensorFlow function that is in charge of the training process. This function receives two inputs, ie. two sequences, which are defined in the signature. \n",
    "\n",
    "In the beginning we need to create masks for the Encoder and Decoder. They are passed to the transformer function. Then we utilize GradientTape and make forward passes thru the Transformer.\n",
    "\n",
    "We pick up the predictions and use them to calculate loss. For that, we use the padded function we defined previously. Once that is done, we utilize the optimizer and modify the Transformer's trainable parameters.\n",
    "\n",
    "In the end we call training_loss and training_accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [tf.TensorSpec(shape=(None, None, None), dtype=tf.float32), # input_embeddings [batch, tokens, embeddings]\n",
    "                        tf.TensorSpec(shape=(None, None),       dtype=tf.int32),   # input_tokens     [batch, tokens]\n",
    "                        tf.TensorSpec(shape=(None, None),       dtype=tf.int32)]   # target_tokens    [batch, tokens]\n",
    "                       #tf.TensorSpec(shape=(),                 dtype=tf.bool)]    # apply_embedding, scalar boolean\n",
    "#input_signature=train_step_signature\n",
    "# the @tf.function decorator converts regular (eager) python code, which is easy to debug, \n",
    "# to graph based code, which is performance optimized.\n",
    "@tf.function(input_signature = train_step_signature)\n",
    "def train_step(input_embeddings, input_tokens, target_tokens):\n",
    "    \n",
    "    # Create masks\n",
    "    encoder_padding_mask = maskHandler.padding_mask(inputs=input_tokens)\n",
    "    decoder_padding_mask = maskHandler.padding_mask(inputs=input_tokens)\n",
    "    \n",
    "    look_ahead_mask             = maskHandler.look_ahead_mask(tf.shape(target_tokens)[1])\n",
    "    decoder_target_padding_mask = maskHandler.padding_mask(target_tokens)\n",
    "    combined_mask               = tf.maximum(decoder_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    # Run training step\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # execute training\n",
    "        predictions, _ = transformer(inputs               = input_embeddings,\n",
    "                                     training             = True, \n",
    "                                     encoder_padding_mask = encoder_padding_mask,\n",
    "                                     look_ahead_mask      = combined_mask, \n",
    "                                     decoder_padding_mask = decoder_padding_mask,\n",
    "                                     apply_embedding_enc  = False, # If already inputting embeddings, then apply no further embedding.\n",
    "                                     apply_embedding_dec  = False) # Else, if inputting tokens, then must use embedding layer for both encoder and decoder\n",
    "        # get loss\n",
    "        total_loss = padded_loss_function(targets=target_tokens, predictions=predictions)\n",
    "\n",
    "    # get gradients, d.weights/d.loss, ie gradient of losses w.r.t weights\n",
    "    gradients = tape.gradient(target=total_loss, sources=transformer.trainable_variables)\n",
    "\n",
    "    # apply gradients to variables (weights)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    # get loss and accuracy\n",
    "    training_loss(total_loss)\n",
    "    training_accuracy(target_tokens, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch=0, Percent complete=0.0%, Avg Loss Thus Far=12.268033\n",
      "Epoch=0, Percent complete=11.1%, Avg Loss Thus Far=5.329914\n",
      "Epoch=0, Percent complete=22.2%, Avg Loss Thus Far=4.973799\n",
      "Epoch=0, Percent complete=33.4%, Avg Loss Thus Far=4.806954\n",
      "Epoch=0, Percent complete=44.5%, Avg Loss Thus Far=4.708857\n",
      "Epoch=0, Percent complete=55.6%, Avg Loss Thus Far=4.644617\n",
      "Epoch=0, Percent complete=66.7%, Avg Loss Thus Far=4.599178\n",
      "Epoch=0, Percent complete=77.8%, Avg Loss Thus Far=4.565406\n",
      "Epoch=0, Percent complete=88.9%, Avg Loss Thus Far=4.538999\n",
      "Epoch 0 Loss 4.3410 Accuracy 0.0406\n",
      "Epoch=1, Percent complete=0.0%, Avg Loss Thus Far=8.478209\n",
      "Epoch=1, Percent complete=11.1%, Avg Loss Thus Far=4.302454\n",
      "Epoch=1, Percent complete=22.2%, Avg Loss Thus Far=4.292505\n",
      "Epoch=1, Percent complete=33.4%, Avg Loss Thus Far=4.286272\n",
      "Epoch=1, Percent complete=44.5%, Avg Loss Thus Far=4.282969\n",
      "Epoch=1, Percent complete=55.6%, Avg Loss Thus Far=4.280110\n",
      "Epoch=1, Percent complete=66.7%, Avg Loss Thus Far=4.277248\n",
      "Epoch=1, Percent complete=77.8%, Avg Loss Thus Far=4.274969\n",
      "Epoch=1, Percent complete=88.9%, Avg Loss Thus Far=4.272931\n",
      "Epoch 1 Loss 4.2535 Accuracy 0.0416\n",
      "Epoch=2, Percent complete=0.0%, Avg Loss Thus Far=8.246263\n",
      "Epoch=2, Percent complete=11.1%, Avg Loss Thus Far=4.246847\n",
      "Epoch=2, Percent complete=22.2%, Avg Loss Thus Far=4.247508\n",
      "Epoch=2, Percent complete=33.4%, Avg Loss Thus Far=4.245807\n",
      "Epoch=2, Percent complete=44.5%, Avg Loss Thus Far=4.244810\n",
      "Epoch=2, Percent complete=55.6%, Avg Loss Thus Far=4.244240\n",
      "Epoch=2, Percent complete=66.7%, Avg Loss Thus Far=4.243204\n",
      "Epoch=2, Percent complete=77.8%, Avg Loss Thus Far=4.241892\n",
      "Epoch=2, Percent complete=88.9%, Avg Loss Thus Far=4.240657\n",
      "Epoch 2 Loss 4.2311 Accuracy 0.0422\n",
      "Epoch=3, Percent complete=0.0%, Avg Loss Thus Far=8.530712\n",
      "Epoch=3, Percent complete=11.1%, Avg Loss Thus Far=4.249755\n",
      "Epoch=3, Percent complete=22.2%, Avg Loss Thus Far=4.235297\n",
      "Epoch=3, Percent complete=33.4%, Avg Loss Thus Far=4.228663\n",
      "Epoch=3, Percent complete=44.5%, Avg Loss Thus Far=4.224215\n",
      "Epoch=3, Percent complete=55.6%, Avg Loss Thus Far=4.221329\n",
      "Epoch=3, Percent complete=66.7%, Avg Loss Thus Far=4.219653\n",
      "Epoch=3, Percent complete=77.8%, Avg Loss Thus Far=4.218632\n",
      "Epoch=3, Percent complete=88.9%, Avg Loss Thus Far=4.217787\n",
      "Epoch 3 Loss 4.2117 Accuracy 0.0427\n",
      "Epoch=4, Percent complete=0.0%, Avg Loss Thus Far=9.164587\n",
      "Epoch=4, Percent complete=11.1%, Avg Loss Thus Far=4.229266\n",
      "Epoch=4, Percent complete=22.2%, Avg Loss Thus Far=4.220439\n",
      "Epoch=4, Percent complete=33.4%, Avg Loss Thus Far=4.216450\n",
      "Epoch=4, Percent complete=44.5%, Avg Loss Thus Far=4.213203\n",
      "Epoch=4, Percent complete=55.6%, Avg Loss Thus Far=4.211480\n",
      "Epoch=4, Percent complete=66.7%, Avg Loss Thus Far=4.210286\n",
      "Epoch=4, Percent complete=77.8%, Avg Loss Thus Far=4.209460\n",
      "Epoch=4, Percent complete=88.9%, Avg Loss Thus Far=4.208904\n",
      "Epoch 4 Loss 4.2047 Accuracy 0.0425\n",
      "Time to train =  1.0  hours\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# parameters for training\n",
    "dataset     = train\n",
    "batch_size  = 32\n",
    "epochs      = 5\n",
    "data_length = len(dataset)\n",
    "input_item  = 'effect_idxs'\n",
    "target_item = 'cause_idxs'\n",
    "\n",
    "# parameters for reporting batch results\n",
    "batch_qty  = len(dataset)//batch_size + 1\n",
    "report_freq= 10 # eg, report each 10% through the epoch (at 0%, 10%, 20% etc)\n",
    "report_set = np.linspace(1,batch_qty, report_freq)\n",
    "report_set = [round(x) for x in report_set]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    training_loss.reset_states()\n",
    "    training_accuracy.reset_states()\n",
    "\n",
    "    # reset the batch starting position\n",
    "    batch_start = -1\n",
    "    \n",
    "    # initialise the epoch losses at 0\n",
    "    sum_loss_epoch = 0\n",
    "\n",
    "    # An epoch = randomly select a fixed number of batches from the data\n",
    "    # We don't cycle though the data in the same order each time, so we 'shuffle' the data\n",
    "    for batch_instance in range(batch_qty) : \n",
    "\n",
    "        # randomly select the next batch start\n",
    "        while batch_start + batch_size > data_length or batch_start < 0:\n",
    "            batch_start = random.randrange(0,data_length,1)\n",
    "            batch_end   = batch_start + batch_size\n",
    "        \n",
    "        ## GET BATCH INPUTS\n",
    "\n",
    "        # we prebuilt the embeddings from BERT and padded tokens in an earlier chunk...\n",
    "        input_embeddings = train_input_embed[batch_start:batch_end]\n",
    "        input_tokens     = train_input_token[batch_start:batch_end]\n",
    "\n",
    "        ## GET BATCH TARGETS\n",
    "        # get target tokens from pandas \n",
    "        target_tokens = dataset[target_item][batch_start:batch_end].tolist().copy()\n",
    "        \n",
    "        # ensure target tokens are padded and in numpy format\n",
    "        target_tokens = pad_tokens(batch_of_tokens = target_tokens)\n",
    "\n",
    "        # ensure all starting tokens, [CLS]=101, and ending tokens, [SEP]=102, are considered equivalent to padding because they are always the same\n",
    "        # if these are not masked then the training tends to fixate on them and returns only a starting and ending token\n",
    "        input_tokens[:, 0]  = 0 \n",
    "        input_tokens[:,-1]  = 0\n",
    "        target_tokens[:, 0] = 0 \n",
    "        target_tokens[:,-1] = 0 \n",
    "\n",
    "        ## EXECUTE TRAINING\n",
    "        train_step(input_embeddings = tf.convert_to_tensor(input_embeddings, dtype=tf.float32),\n",
    "                   input_tokens     = tf.convert_to_tensor(input_tokens,     dtype=tf.int32),\n",
    "                   target_tokens    = tf.convert_to_tensor(target_tokens,    dtype=tf.int32))\n",
    "                  #apply_embedding  = tf.convert_to_tensor(False,            dtype=tf.bool))\n",
    "    \n",
    "        # keep record of sum of epoch loss thus far\n",
    "        sum_loss_epoch = sum_loss_epoch + training_loss.result()\n",
    "\n",
    "        # print batch results each x% thru the data\n",
    "        if batch_instance in report_set:\n",
    "            print('Epoch={}, Percent complete={:.1f}%, Avg Loss Thus Far={:.6f}'.format(epoch, batch_instance/batch_qty*100, sum_loss_epoch/batch_instance))\n",
    "\n",
    "        # reset batch start, so it will be randomly selected\n",
    "        batch_start = -1\n",
    "\n",
    "    # with training bar...\n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch, training_loss.result(), training_accuracy.result()))\n",
    "\n",
    "# report how long the training took\n",
    "print('Time to train = ',round((time.time() - start_time)/3600,1),' hours')"
   ]
  },
  {
   "source": [
    "## Model Summary\n",
    "\n",
    "Transformer models are famously large, and this is no exception. \n",
    "This quantity of trainable parameters was always going to take a long time to train:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"transformer_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nencoder_3 (Encoder)          multiple                  15760384  \n_________________________________________________________________\ndecoder_3 (Decoder)          multiple                  25209856  \n_________________________________________________________________\ndense_259 (Dense)            multiple                  23471418  \n=================================================================\nTotal params: 64,441,658\nTrainable params: 64,441,658\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## That took a few hours, so let's save the weights\n",
    "# There are 343Mb of weights when saved in hdf format!\n",
    "\n",
    "transformer.save_weights(os.path.join(path, 'transformer_weights_20201201.h5'))\n"
   ]
  },
  {
   "source": [
    "## Sample Performance on Training data\n",
    "\n",
    "Let's see how the model performs on some TRAINING examples before we evaluate the model on Test data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer transformer_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "=====================================\n",
      "Example No      =  0\n",
      "Input Effect    =  The school said that the relocation is needed \n",
      "Target Cause    =   the existing premises were overcrowded, and there are safety concerns\n",
      "Predicted Cause =  the the the the the the the the the the the the the the the the the the the the the the the\n",
      "=====================================\n",
      "Example No      =  1\n",
      "Input Effect    =  In Cairo, Egypt's King Farouk asked her to sing; she refused \n",
      "Target Cause    =   Egypt had not recognized Free France and remained neutral\n",
      "Predicted Cause =  he he he he he he he he he he he he he he he he he he he he he he he\n",
      "=====================================\n",
      "Example No      =  2\n",
      "Input Effect    =  Kayseri Erciyesspor gained qualification to the UEFA Cup \n",
      "Target Cause    =   of Beikta' qualification to the UEFA Champions League Second Qualifying Round\n",
      "Predicted Cause =  the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "## GET SAMPLE INPUTS\n",
    "# get input tokens from pandas\n",
    "pandas_sample_eval = train.sample(n=3).reset_index(drop=True)\n",
    "input_tokens_eval  = pandas_sample_eval[input_item].tolist().copy()\n",
    "target_tokens_eval = np.array(pandas_sample_eval[target_item].tolist().copy())\n",
    "\n",
    "# map those input tokens to embeddings by using BERT\n",
    "# also get the input_tokens returned as numpy\n",
    "input_embeddings_eval, input_tokens_eval = get_embeddings_and_tokens(batch         = input_tokens_eval, \n",
    "                                                                     BERT_embedder = BERT_embedder, \n",
    "                                                                     negate_padding= False)\n",
    "\n",
    "# we have a subclassed model, so can't easily use the keras model.predict() method.\n",
    "# Instead we execute our model with a function call...\n",
    "# Happily, we can use the same masks as during training\n",
    "encoder_padding_mask_eval = maskHandler.padding_mask(inputs=input_tokens_eval)\n",
    "decoder_padding_mask_eval = maskHandler.padding_mask(inputs=input_tokens_eval)\n",
    "\n",
    "look_ahead_mask_eval             = maskHandler.look_ahead_mask(tf.shape(input_tokens_eval)[1])\n",
    "decoder_target_padding_mask_eval = maskHandler.padding_mask(input_tokens_eval)\n",
    "combined_mask_eval               = tf.maximum(decoder_target_padding_mask_eval, look_ahead_mask_eval)\n",
    "\n",
    "predictions, _ = transformer(inputs               = input_embeddings_eval,\n",
    "                             training             = False, \n",
    "                             encoder_padding_mask = encoder_padding_mask_eval, \n",
    "                             look_ahead_mask      = combined_mask_eval, \n",
    "                             decoder_padding_mask = decoder_padding_mask_eval,\n",
    "                             apply_embedding_enc  = False,  # We will feed the transformer with BERT embeddings, not tokens. So encoder needs no embedding.\n",
    "                             apply_embedding_dec  = False)  # Decoder should always use a embedding layer\n",
    "\n",
    "# predictions are softmax'ed probabilities for each token\n",
    "# prediction shapes are [batch, tokens_per_clause, vocabulary_size]\n",
    "# so we must select the most probable vocabulary member for each token\n",
    "predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# we can convert predicted token indexes back to readable(ish) tokens using the BERT tokenizer, \n",
    "predictions_text_dim2 = [tokenizer.convert_ids_to_tokens(prediction) for prediction in predictions]\n",
    "\n",
    "# The tokenzier returns an array of one word for each token, these need joining together into a single sentence\n",
    "predictions_text = [' '.join([x for x in prediction_text]) for prediction_text in predictions_text_dim2]\n",
    "\n",
    "# remove token separations ' ##' and start and finish tokens, [CLS] and [SEP]\n",
    "predictions_text = [prediction_text.replace(' ##', '')   for prediction_text in predictions_text]\n",
    "predictions_text = [prediction_text.replace('[CLS]', '') for prediction_text in predictions_text]\n",
    "predictions_text = [prediction_text.replace('[SEP]', '') for prediction_text in predictions_text]\n",
    "\n",
    "# print predictions beside expected values\n",
    "for i in range(len(predictions_text)):\n",
    "    input_effect   = pandas_sample_eval['effect'][i]\n",
    "    target_cause   = pandas_sample_eval['cause'][i]\n",
    "    predicted_cause= predictions_text[i]\n",
    "    print('=====================================')\n",
    "    print('Example No      = ', i)\n",
    "    print('Input Effect    = ', input_effect)\n",
    "    print('Target Cause    = ', target_cause)\n",
    "    print('Predicted Cause = ', predicted_cause)\n"
   ]
  },
  {
   "source": [
    "## Text Degeneration\n",
    "\n",
    "The model is repeating common words and failing to learn. This is common in NLP. It is related to the decoder and/or the learning rate. \n",
    "There are a number of approaches which can be used to rectify the problem with the decoder:\n",
    "\n",
    "https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb\n",
    "\n",
    "Repetition penalty looks like a useful candidate.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}